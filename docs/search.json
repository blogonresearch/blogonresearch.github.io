[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a “revival” of an old blog of mine on psychological research and methodology (hence the name Blogonresearch). Just anything on research and methodology that are useful to me, and maybe also useful to others.\n– Shu Fai Cheung\nP.S.: This is actually the “second revival” as I switched from Hugo to Quarto in early 2023."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogonresearch",
    "section": "",
    "text": "Proper Standardization in Moderated Regression Using std_selected_boot()\n\n\n\n\n\n\n\nR\n\n\nregression\n\n\nmoderation\n\n\nstdmod\n\n\nstandardized\n\n\nbootstrapping\n\n\nconfidence-intervals\n\n\n\n\n\n\n\n\n\n\n\nSaturday, July 1, 2023\n\n\nShu Fai Cheung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStandardize Variables Except For Dummy Variables, Using std_selected_boot()\n\n\n\n\n\n\n\nR\n\n\nregression\n\n\ncategorical variables\n\n\nstdmod\n\n\nstandardized\n\n\nbootstrapping\n\n\nconfidence-intervals\n\n\n\n\n\n\n\n\n\n\n\nSunday, June 25, 2023\n\n\nShu Fai Cheung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrap Confidence Intervals for Standardized Solution in lavaan\n\n\n\n\n\n\n\nR\n\n\nlavaan\n\n\nbootstrapping\n\n\nconfidence-intervals\n\n\nsemhelpinghands\n\n\nstandardized\n\n\n\n\n\n\n\n\n\n\n\nWednesday, September 28, 2022\n\n\nShu Fai Cheung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow Options Set by lavaan\n\n\n\n\n\n\n\nR\n\n\nlavaan\n\n\nsemhelpinghands\n\n\n\n\n\n\n\n\n\n\n\nMonday, September 26, 2022\n\n\nShu Fai Cheung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Own Style in R\n\n\n\n\n\n\n\nR\n\n\ncode-style\n\n\n\n\n\n\n\n\n\n\n\nSaturday, September 24, 2022\n\n\nShu Fai Cheung\n\n\n\n\n\n\n  \n\n\n\n\nCustomize R GUI For Windows\n\n\n\n\n\n\n\nR\n\n\ngui\n\n\n\n\n\n\n\n\n\n\n\nTuesday, September 20, 2022\n\n\nShu Fai Cheung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne Function or Many Functions\n\n\n\n\n\n\n\nR\n\n\ngui\n\n\n\n\n\n\n\n\n\n\n\nWednesday, September 14, 2022\n\n\nShu Fai Cheung\n\n\n\n\n\n\n  \n\n\n\n\nPlotting Moderation Effects With ggplot2\n\n\n\n\n\n\n\nR\n\n\nmoderation\n\n\n\n\n\n\n\n\n\n\n\nSunday, September 11, 2022\n\n\nShu Fai Cheung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReviving an Old Blog\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nSunday, September 11, 2022\n\n\nShu Fai Cheung\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bootstrap_confidence_intervals_for_standardized_solution_in_lavaan/index.html",
    "href": "posts/bootstrap_confidence_intervals_for_standardized_solution_in_lavaan/index.html",
    "title": "Bootstrap Confidence Intervals for Standardized Solution in lavaan",
    "section": "",
    "text": "lavaan supports bootstrap confidence intervals for free and user-defined parameters. This is useful especially for parameter estimates that may not be approximately normally distributed unless the sample size is very large.\nHowever, it is known, though not well-known enough in my opinion, that, even if bootstrap confidence intervals are requested, the confidence intervals reported in the standardized solution are not bootstrap confidence intervals as in tools like PROCESS for standardized effects like standardized indirect effects, but are symmetric delta-method confidence intervals based on the bootstrap sampling variance-covariance matrix.\nLet’s use a sample dataset for illustration:\n\n# Create the data\nset.seed(860541)\nn <- 100\nx <- rnorm(n)\nm <- .4 * x + rnorm(n, 0, sqrt(1 - .3^2))\ny <- .4 * m + rnorm(n, 0, sqrt(1 - .4^2))\ndat <- data.frame(x = 10 * x, m = 2 * m, y = 3 * y)\n\nWe specify a simple regression model:\n\nlibrary(lavaan)\n\nThis is lavaan 0.6-13\nlavaan is FREE software! Please report any bugs.\n\nmod <-\n\"\nm ~ a * x\ny ~ b * m + cp * x\nab := a * b\n\"\n\n… and fit it with bootstrap confidence intervals (2023-01-28: Code and results updated for lavaan 0.6-13, iseed is used instead of set.seed()):\n\nfit <- sem(mod, data = dat, fixed.x = FALSE,\n           se = \"boot\", bootstrap = 2000,\n           iseed = 8970)\n\n\n\n\nLet’s focus on the confidence intervals of the indirect effect:\n\nest <- parameterEstimates(fit)\nstd <- standardizedSolution(fit)\n# Unstandardized\nest[7, ]\n\n  lhs op rhs label   est    se     z pvalue ci.lower ci.upper\n7  ab := a*b    ab 0.025 0.015 1.686  0.092    0.001    0.059\n\n# Standardized\nstd[7, ]\n\n  lhs op rhs label est.std    se     z pvalue ci.lower ci.upper\n7  ab := a*b    ab   0.088 0.049 1.774  0.076   -0.009    0.185\n\n\nThey lead to different conclusions.\nAs shown below, the confidence interval of the unstandardized indirect effect is percentile confidence interval that is asymmetric, as expected:\n\nest[7, c(\"ci.lower\", \"ci.upper\")] - est[7, \"est\"]\n\n     ci.lower   ci.upper\n7 -0.02364024 0.03392409\n\n\nHowever, the confidence interval of the standardized indirect effect is symmetric:\n\nstd[7, c(\"ci.lower\", \"ci.upper\")] - std[7, \"est.std\"]\n\n     ci.lower   ci.upper\n7 -0.09699904 0.09699904\n\n\n\n\n\nThis behavior has been discussed in the Google group forlavaan and so is known, but not “well-known” because I met many users who were not aware of this, especially when they use bootstrapping to get the confidence intervals for indirect effects but found that the confidence intervals of unstandardized and standardized indirect effect led to different conclusions, as in the example above.\nA solution already exists in lavaan. Users can use bootstrapLavaan() and get the bootstrap confidence intervals for many results, including the output of standardized solution.\nWe first define a function to extract the standardized indirect effect:\n\nfct <- function(fit) {\n    lavaan::standardizedSolution(fit)[7, \"est.std\"]\n  }\n\nWe then update the fit object to disable standard error because we only need the point estimates and then call bootstrapLavaan():\n\nfit0 <- update(fit, se = \"none\")\nfit_boot <- bootstrapLavaan(fit0, R = 2000, FUN = fct, iseed = 8970)\n\n\n\n\nThe percentile confidence interval can then be formed by quantile().\n(Note that lavaan() does not use quantile() but use the approach by boot.ci(). The resulting interval can be slightly different from that by quantile().)\n\nquantile(fit_boot[, 1], c(.025, .975))\n\n       2.5%       97.5% \n0.004372947 0.196203435 \n\n\nHowever, this is inconvenient because we need to write custom function, and bootstrapping was done twice unless we store both the unstandardized and standardized solutions in the custom function used when calling bootstrapLavaan().\nI wrote the function standardizedSolution_boot_ci(), available in the package semhelpinghands, for this particular case that I sometimes encounter:\n\nA model is already fitted with se = \"boot\" and so bootstrap confidence intervals are already available for the unstandardized estimates.\nI want to get the bootstrap confidence intervals for the standardized solution without doing the bootstrapping again.\n\nThis would be useful to me because some of my projects involve large samples with missing data. and bootstrapping takes appreciable time even with parallelization.\nThis is how to use this function:\n\nlibrary(semhelpinghands)\nstd_boot <- standardizedSolution_boot_ci(fit)\n# -c(9, 10) is used to remove the delta-method CIs from\n# the printout\nstd_boot[, -c(9, 10)]\n\n  lhs op rhs label est.std    se      z pvalue boot.ci.lower boot.ci.upper\n1   m  ~   x     a   0.232 0.105  2.213  0.027         0.015         0.425\n2   y  ~   m     b   0.379 0.083  4.541  0.000         0.204         0.541\n3   y  ~   x    cp   0.103 0.092  1.117  0.264        -0.079         0.281\n4   m ~~   m         0.946 0.048 19.527  0.000         0.819         0.999\n5   y ~~   y         0.828 0.073 11.403  0.000         0.660         0.940\n6   x ~~   x         1.000 0.000     NA     NA            NA            NA\n7  ab := a*b    ab   0.088 0.049  1.774  0.076         0.004         0.196\n\n\nThe boot.ci intervals are “true” bootstrap confidence intervals, formed from the bootstrap estimates. The bootstrap confidence interval for the standardized indirect effect ([0.004, 0.196]) and that for the unstandardized indirect effect ([0.001, 0.059]) now lead to the same conclusion.\nstandardizedSolution_boot_ci() works like standardizedSolution(), but extracts the stored bootstrap estimates, get the standardized solution from each set of estimates, and use them to form the bootstrap confidence intervals for the standardized solution.\nBy default, the bootstrap standardized solution is also stored in the attribute boot_est_std. They can be extracted to examine the distribution. For example, the bootstrap standardized indirect effects can be extracted and plotted:\n\nstd_boot_est <- attr(std_boot, \"boot_est_std\")\nstd_indirect_boot_est <- std_boot_est[, 7]\nhist(std_indirect_boot_est)\n\n\n\nqqnorm(std_indirect_boot_est)\nqqline(std_indirect_boot_est)\n\n\n\n\nThis function is simple to use, at least for me. No need to write custom function, and no need to do bootstrapping twice. In most cases, I don’t even need to specify any additional arguments.\nMore about this function can be found in the vignette for standardizedSolution_boot_ci().\nIf any bug in standardizedSolution_boot_ci() was found, I would appreciate submitting it as a GitHub issue."
  },
  {
    "objectID": "posts/customizes_r_gui_for_windows/index.html",
    "href": "posts/customizes_r_gui_for_windows/index.html",
    "title": "Customize R GUI For Windows",
    "section": "",
    "text": "Many people use RStudio, and some even mistaken RStudio as R. I mainly work on Windows machines and I did try RStudio for a while a few years ago. However, I switched back to default R GUI for Windows that comes with R for Windows and used it along with light-weight code editors, for personal reasons. RStudio is good, but has many features that I don’t need. Although I now use VS Code as my main IDE for R, I still use R GUI for Windows a lot. It is light-weight, came with base installation, and is customizable. Simple but good enough for some tasks.\n\n\n\nR Default GUI\n\n\nThe console above is not the default one. I like dark theme and single-document interface (SDI). I keep only a limited numbers of windows on my desktop and I never group windows in the taskbar. SDI is much more efficient for me to locate the window I need.\nThe configuration can be set in Edit->Preferences:\n\n\n\nRgui configuration\n\n\nThe configuration is saved in the Rconsole file in the folder etc in R’s installation folder. Whenever I upgrade to a new version of R, I simply copy this file to the same folder in the new installation of R to have my preferred configuration. No need to set the configuration again. (I’ve just found that I haven’t changes this configuration for over five years!)\nAnother feature I like is customizing the menu bar. I use devtools a lot, and would love to call them from the pulldown menu … well, not really. I rarely use the mouse to access the pulldown menu. I use the keyboard most of the time.\n\n\n\nR GUI pulldown menu\n\n\nTo add a menu, we can use a script and two functions.\nThe following function add a menu called devtools:\n\nutils::winMenuAdd(\"devtools\")\n\nAfter a menu is added, items can be added by utils::winMenuAddItem(). For example, the following call add check to the menu devtools. If selected, the call devtools::check() will be executed:\n\nutils::winMenuAddItem(\"devtools\",\n                      \"check\",\n                      \"devtools::check()\")\n\nAs the screenshot above showed, most of the items I added to devtools are those functions (menu items) in RStudio (as far as I recalled … as I haven’t used RStudio to develop package for a long time).\nMost common tasks that can be done through R code can be converted to a menu item. For example, I don’t like using the pulldown menu to change working directory. I can quickly copy the path to a folder using keyboard only (alt-D and then control-C in Explorer). Therefore, I have the following menu item added to the R GUI:\n\nutils::winMenuAddItem(\"Utils\",\n  \"setwd from clipboard\",\n  \"setwd(readClipboard());getwd()\")\n\nsetwd(readClipboard()) changes the working directory to the folder I copied to clipboard. getwd() is added just to confirm the change.\nI sometimes open several instances of R GUI. To differentiate them, I use different color schemes:\n\n\n\nFour instances of R GUI\n\n\nThis can be done inside R GUI too:\n\nutils::winMenuAddItem(\"Style\",\n  \"Lime Green and Light Golden\",\n  \"utils::loadRconsole('D:/My_Settings/Rconsole_lime_green_and_light_golden')\")\n\nutils::loadRconsole() is used to load the R GUI configuration stored in a file. This file can be created by saving the settings in the R GUI configuration dialog above to a file. In the above example, the settings are stored in the file Rconsole_lime_green_and_light_golden. I have four such files, storing distinct color schemes.\nLike Rconsole for the color scheme and SDI, I would like to have custom menus when I start R GUI. This can be done by adding code like the following to the Rprofile.site file in the etc folder:\n\nif ((Sys.getenv(\"RS_LOCAL_PEER\") == \"\") && (.Platform$GUI == \"Rgui\")) {\n    tryCatch(source(\"D:/My_Settings/add_style_menu.R\"), error = function(e) e)\n  }\n\nI wrote this a long long time ago and I cannot recall their purposes. I believe Sys.getenv(\"RS_LOCAL_PEER\") == \"\" is used to check whether a session is launched in RStudio. .Platform$GUI == \"Rgui\" is used to check whether an R session is launched by the default R GUI. I still occasionally use RStudio and the menus I created should not be added if an R session is launched inside RStudio. I used tryCatch() just in case there are issues that I overlooked.\nSo, whenever I install a new copy or version of R, I just copy and paste the customized Rconsole and Rprofile.site files to etc. I can then happily have my preferred environment. Having the code for menu groups stored in other files allow me to update them without editing Rprofile.site.\nHope you find these tips useful … if you are R GUI users like me. :)"
  },
  {
    "objectID": "posts/my_own_style_in_r/index.html",
    "href": "posts/my_own_style_in_r/index.html",
    "title": "My Own Style in R",
    "section": "",
    "text": "Although I have written programs since I were a high school student, when computer monitor could only display one color, I have no formal training in programming, and I rarely worked with others in developing a solution until recently. The problem: I did not write with a consistent and professional style. I am pretty sure that my code will look “ugly” to professional programmers.\nThat said, I do have a loose style, one that suits my own situation:\n\nI work on small screen frequently, sometimes even on my mobile phone. I wrote the documentation of some packages on my mobile phone, and even this post was largely drafted on my phone.\nMy main tasks are research and teaching, among other tasks. I can easily forget some style rules I set for coding. I need something simple and easy to remember.\nIdeally, code should be easy, or at least not too difficult, to comprehend by future me with minimal comments. What looks nature to me myself is of the top priority.\n\nSo, this is my style, with me as the main user and reader:\n\nFor code, I use a line width of 60 to 70. For documentation, I am more aggressive and use a line width of 40.\nFor the same reason, I use two spaces for indentation. A four-space indentation is too “expensive” to me.\n\n\ntmpfct <- function(x) {\n    x^2\n  }\n\n\nI like using double-click to select a name. This does not work if periods are used inside a name. Therefore, I no longer use periods in a name, except for S3 methods. If necessary, I use underscores.\n\n\nthis_is_long_name <- 1\n\n\nI found it difficult to remember the case I used for a name. So, I stick to lowercase letters unless I am very certain that I can remember that I used uppercase letters.\n\nI don’t like camel case. It is OK for language that is not case sensitive, like Visual Basic and SPSS syntax commands, but is inconvenient for case sensitive languages like R.\n\n# I don't like camel case.\nthisIsNotWhatIDo <- 1\n# I prefer this:\nthis_is_what_i_do <- 1\n\n\nLong function names are acceptable with me. With autocompletion in many IDEs, it is not important to use short names. Being easy to remember part of a name is important. An abbreviation is not easy to remember unless it is commonly used (e.g., SD).\n\n\n# This is easy to remember\nfactor_loadings()\n# These variants are not\nfload()\nfacload()\nfacload()\nfl()\n\n\nI wrote stuff in Python occasionally. I like the Python style indentation, which is easy to read. So I use that style for my R code too.\n\n\nfor (j in 1:10) {\n    # Do something\n  }\nif (x == 1) {\n    # Do something\n  } else {\n    # Do something else\n  }\n\n\nI never, ever, use any automatic stylers to reformat code. They make changes that are tracked by Git but have nothing to do with the content. I may use them, but only when finalizing the code.\nThis is also why I care little about word wrap. Irregular line widths are acceptable for me.\nExtra whitespace are OK with me. Readability is the main goal.\n\n\n# I may do this:\nx  <-   1\ny0 <- 100\n\n\nI use double quotes for string literals. I have to use two keys … but the habit is too difficult to break that I don’t bother changing it.\nThe last “rule”: I can break any of the rules, as long as the code is readable without the need to know any rules.\n\nI also have a GitHub repo for my personal style, in case I forgot the rules:\nhttps://github.com/sfcheung/rstylesf\nSo, please pardon me if you find my code for packages at odd with professional style. I myself is the main reader and maintainer of the packages. What work for me matters."
  },
  {
    "objectID": "posts/one_function_or_many_functions/index.html",
    "href": "posts/one_function_or_many_functions/index.html",
    "title": "One Function or Many Functions",
    "section": "",
    "text": "I am thinking about the differences between R and SPSS in doing analysis: one function for one analysis, and several functions for one analysis.\nThough not always the case, in R, it is common to do an analysis using several functions. One of them is the “main” function that do the main analysis. Other functions are used to extract information or compute other statistics.\nFor example, to do multiple regression, this is what we may do:\n\n# Create a Test Dataset\nset.seed(586045)\nn <- 100\ndat <- data.frame(x1 = rnorm(n, 5, 1),\n                  x2 = rnorm(n, 10, 2))\ndat$y <- 2 * dat$x1 + 1 * dat$x2 + rnorm(n, 0, 15)\n# Do regression\nlm_out <- lm(y ~ x1 + x2, dat)\n\nThe main analysis is done by lm().\nWe then use other functions on the output of lm(). For example, we can use summary() to print commonly requested results:\n\nsummary(lm_out)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.807 -10.733   0.153   9.472  37.611 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -20.7634    12.0270  -1.726  0.08746 . \nx1            2.8688     1.6483   1.740  0.08495 . \nx2            2.5771     0.8555   3.012  0.00331 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.69 on 97 degrees of freedom\nMultiple R-squared:  0.1113,    Adjusted R-squared:  0.09295 \nF-statistic: 6.072 on 2 and 97 DF,  p-value: 0.003276\n\n\nConfidence intervals and variance-covariance matrix of the estimates can be obtained by confint() and vcov():\n\nconfint(lm_out)\n\n                  2.5 %   97.5 %\n(Intercept) -44.6335974 3.106785\nx1           -0.4025881 6.140222\nx2            0.8791251 4.275063\n\nvcov(lm_out)\n\n            (Intercept)            x1           x2\n(Intercept)  144.647827 -13.347713331 -7.460851584\nx1           -13.347713   2.716868988 -0.005532551\nx2            -7.460852  -0.005532551  0.731913160\n\n\nThere are many functions for other statistics, such as influential statistics and model comparison.\nIn SPSS, to do analysis, we usually use a dialog box from the pull down menu, select variables, check some checkboxes, use some buttons to open other dialog boxes and set other options, click OK, and all the requested results are in the output.\nI used to think that this approach is due to the graphical user interface (GUI), which is the strength of SPSS. I forgot that (a) the GUI is a “syntax generator,” and (b) the format of SPSS syntax we have nowadays is very similar to that in SPSS before it has a GUI. Actually, when I first learned SPSS in 90’s, I did not even have access to a PC version with text menu. Syntax command was the only way to do analysis in SPSS. For example, REGRESSION is the command, and all the checkboxes and options are values for subcommands, like arguments in R functions.\nSo, the common way we do analysis in SPSS, with one command for one analysis, is not due to the GUI. It has always been this way, at least in the version I used in early 90’s, before systems like Windows became popular.\nSo, for an analysis, such as multiple regression, one function, or many functions?\nWhen I write functions or develop packages, I generally adopt the do-one-thing-and-do-it-well principle, though what constitutes “one thing” is not always clear. This principle make it easy for me to write, debug, and maintain a function or package.\nHowever, for users who are used to using GUI, using one function to do many things in an analysis is conceptually similar to using a dialog box, thought without the dialog box. The many-function approach does not fit well with the experience in using a dialog box.\nIn R, we certainly can write a function that calls other functions, simulating commands like REGRESSION in SPSS.\nSo, I think this is not a debate of which approach is better. In R, we can do both, and let the users do analysis in whatever approach they like. For development, the do-one-thing-and-do-it-well approach is a better approach. However, for users, especially when developing GUI, the one-function approach may be more convenient to the users. The function in the one-function approach, like REGRESSION, is like a wrapper of a collection of functions: an interface to them.\nFor example, we can write an R function similar to REGRESSION in SPSS. In SPSS, if all the default options are what we need, this command is sufficient:\nREGRESSION\n /DEPENDENT y\n /ENTER x1 x2.\nTo request confidence intervals (confint() in R) and the variance-covariance matrix of the estimates (vcov() in R), this will do:\nREGRESSION\n /STATISTICS DEFAULT BCOV CI(95)\n /DEPENDENT y\n /ENTER x1 x2.\nA similar function can be written in R:\n\nregression(data = dat,\n           dep = \"y\",\n           ivs = c(\"x1\", \"x2\"))\n\nWe can write it in a more “R-way”:\n\nregression(data = dat,\n           model = y ~ x1 + x2)\n\nThe default printout is something similar to SPSS. It can be a list of tables (data frames) and a print method for printing the output.\nActually, we can still say that we are adopting the do-one-thing-and-do-it-well approach, although the “one thing” is “an interface to a set of functions.”\nI am not trying to argue that we should use this or that approach. They are not mutually exclusive. I am just wondering how to make using R by writing scripts more accessible to users who are used to GUI, while still keeping the do-one-thing-and-do-it-well principle. Writing these kinds of wrappers may also make it easier to create GUIs for them. For example, as long as ... is not used, a generic function can be developed to check the arguments of a function using its definition and then automatically generate a dialog box for it. For a wrapper with a lot of arguments, a configuration file can be used to customize the dialog box.\nP.S.: jamovi is already doing something similar. Behind the dialog boxes are kind of wrapper functions. However, though can be used in console, the modules are, naturally, supposed to be used inside jamovi."
  },
  {
    "objectID": "posts/plot_mod/index.html",
    "href": "posts/plot_mod/index.html",
    "title": "Plotting Moderation Effects With ggplot2",
    "section": "",
    "text": "There are some R packages that help researchers to plot moderation (interaction) effects: The linear relations between x (independent variable / predictor) and y (dependent variable / outcome variable) for two or more levels of w (moderator). For example, I have been using visreg for multiple regression models fitted by lm() for a long time. It is simple to use and supports both base R graphics and ggplot2. stdmod, which I maintained, also has the function plotmod for plotting simple effects in moderated regression. For structural equation modelling, semTools can be used to plot interaction for latent variables using plotProbe(). plotProbe() can also be used to on observed variables using this workaround.\nHowever, there may be case in which all we need is just two or more lines, and all we have are the simple effects: Two or more sets of intercepts of slopes.\nThis is how to plot the simple effect:\nSuppose we want to plot the simple effects of x on y conditional on w (the moderator). From the output of some functions, we have the slopes and intercepts when w is “Low” or “High”:\nw is “Low”: intercept = 2, slope = 1\nw is “High”: intercept = 3, slope = 2\nWrite a simple function to compute the points\n\n# Simple regression model\nxyline <- function(x, a, b) {a + b * x}\n\nSet the range for x:\n\n# Range of x\nx <- c(0, 10)\n\nCompute the predicted values of y at the lower and upper limit of the range of x, for each level of w, when all other predictors of y in the model, if any, are equal to zero:\n\n# Generate the two points when moderator = \"Low\"\ndat0 <- data.frame(Moderator = \"Low\",\n                   x = x,\n                   y = xyline(x, a = 2, b = 1))\n# Generate the two points when moderator = \"High\"\ndat1 <- data.frame(Moderator = \"High\",\n                   x = x,\n                   y = xyline(x, a = 3, b = 2))\n\nCombine the datasets:\n\ndat <- rbind(dat0, dat1)\ndat\n\n  Moderator  x  y\n1       Low  0  2\n2       Low 10 12\n3      High  0  3\n4      High 10 23\n\n\nDraw the lines using ggplot2:\n\nlibrary(ggplot2)\np <- ggplot(dat, aes(x = x, y = y, color = Moderator)) +\n            geom_line() +\n            scale_color_manual(values = c(\"Low\" = \"blue\", \"High\" = \"red\"))\np\n\n\n\n\n\n\n\n\nThis plot can then be modified as necessary:\n\np2 <- p + xlab(\"Independent Variable\") +\n          ylab(\"Dependent Variable\")\np2\n\n\n\n\n\n\n\n\nThis solution can be used for multiple regression or structural equation modelling.\nThough not as elegant as using packages devoted to plotting moderation effects, this solution may be good enough for some simple scenarios. I believe it can be further improved. However, if we want more, maybe it is better to use packages like semTools and visreg.\nP.S.: This post is based on a suggestion I posted to the Google Group for lavaan."
  },
  {
    "objectID": "posts/show_options_set_by_lavaan/index.html",
    "href": "posts/show_options_set_by_lavaan/index.html",
    "title": "Show Options Set by lavaan",
    "section": "",
    "text": "lavaan is a convenient tool for doing structural equation modelling in R (Rosseel, 2012). One of its strength is having “prepackaged” estimators, which are shortcuts to a set of options, such as “ML”, “MLR”, “MLMVS”, and others (Savalei & Rosseel, 2022). It also tries to set default values for options based on the model and data.\nHowever, probably due to my not-so-good memory, I sometimes forgot what the settings are for a model. Therefore, in the package semhelpinghands, I wrote the function show_more_options() to show some of the settings of the output of lavaan() and its wrappers, such as sem() and cfa().1\nThe function show_more_options() is very easy to use … because it accepts only one argument, the output of lavaan().\nThis is an example based on the example of lavaan::cfa():\nTo show the major options, just pass the output to show_more_options():\nThe column Call shows whether the default setting is used for each row, based of the call used when fitting the model. However, it is not always clear to me what the default values are.\nThe column Actual shows the values extracted by lavaan::lavInspect() or from the Options slot. These are what the default values stand for in the fitted model.\nMany of the entries are either (a) already available in the output of summary(), or (b) can be deduced from the output. However, I would like to have a table for quick reference, hence I wrote this function.\nSuppose \"MLR\" is used as the estimator:\nThe output shows the exact names of the options (e.g., \"robust.huber.white\" and \"yuan.bentler.mplus\"). They can complement the more readable output of summary() if we need to manually set these options, or want to know which values these options refer to when consulting the help page.\nFor example, summary() reports that \"Sandwich\" is the method used for standard errors, and show_more_options() shows that the exact name in the option is \"robust.huber.white\". This is useful because the word \"sandwich\" does not appear in the help page of lavOptions(), while the word \"robust.huber.white\" does. Some users may not know what \"Sandwich\" stands for.\nThis is a dataset for a path model, with missing data:\nSuppose we use only the default options to fit a path model:\nThe output shows that, by default, the mean structure is not modelled, listwise selection is used to handle missing data, and x variables (exogenous covariates, x in this example) are treated as fixed. This can be verified by the parameter estimates, in which the variance of x is a fixed parameter and hence has no standard error and no p-value:\nSuppose we set missing to \"FIML\":\nx variables are still treated as fixed, but now mean structure is modelled (required for FIML, full information maximum likelihood), even though I did not explicitly ask for it.\nOnly options I think are likely needed (by me) are included in the output.2 More may be added in the future. In any case, if other options are needed, they can be retrieved by lavaan::lavInspect() or from the Options slot of the output. In most cases I myself encountered, all I want is a simple function that is easy to remember and no need to set any arguments other than the lavaan output. If I need something else, I will just extract the information myself.\nThis function was inspired by a script I wrote to enumerate the options set by the prepackaged shortcuts. Interested readers can read this thread at the Google Group for lavaan and this gist, to check how options will be set for different combinations of estimator, data, and some other options."
  },
  {
    "objectID": "posts/std_dummy/index.html",
    "href": "posts/std_dummy/index.html",
    "title": "Standardize Variables Except For Dummy Variables, Using std_selected_boot()",
    "section": "",
    "text": "This post shows one simple way to get meaningful standardized regression coefficients in multiple linear regression with dummy variables, with appropriate confidence intervals, using std_selected_boot() from the stdmod package.\n(Note: This post and some related posts, e.g., this one on moderated regression, have sections that are identical or highly similar. This is intentional, to make each article self-contained. Readers do not need to refer to other articles to understand the points.)"
  },
  {
    "objectID": "posts/std_dummy/index.html#example",
    "href": "posts/std_dummy/index.html#example",
    "title": "Standardize Variables Except For Dummy Variables, Using std_selected_boot()",
    "section": "Example",
    "text": "Example\n\nDo Regression As Usual\nSuppose this is the data set:1\n\n\nShow the code for generating data\nset.seed(1453243)\nn <- 200\ngroup <- sample(c(\"Gp1\", \"Gp2\", \"Gp3\"),\n                size = n,\n                prob = c(.30, .20, .50),\n                replace = TRUE)\nx1 <- (rchisq(n, 2) - 2) * 4 + 10\nx2 <- runif(n, 1, 10)\ny <- 10 + 0.45 / 1 * x1 + 4 / 1 * x2 + sapply(group, switch,\n                                   Gp1 = 1,\n                                   Gp2 = 8,\n                                   Gp3 = 6) + rnorm(n, 0, 10)\ndat <- data.frame(x1 = round(x1, 2),\n                  x2 = round(x2, 2),\n                  group,\n                  y = round(y, 2))\nwrite.csv(dat, \"dat.csv\")\n\n\nThis is the data file.\n\nhead(dat)\n\n    x1   x2 group     y\n1 9.97 7.84   Gp1 44.99\n2 2.45 5.11   Gp1 47.27\n3 8.50 8.87   Gp2 60.00\n4 4.10 5.60   Gp1 37.05\n5 8.16 7.18   Gp3 45.51\n6 2.53 9.19   Gp1 54.71\n\n\nThe variables x1 and x2 are continuous predictors. The variable group is a string variable, with three possible values: Gp1, Gp2, and Gp3. The outcome variable is y.\nThis is the regression model:\n\nlm_out <- lm(y ~ x1 + x2 + group, dat)\nsummary(lm_out)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + group, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-42.755  -7.165   0.355   6.597  25.317 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   9.8205     2.2545   4.356 2.14e-05 ***\nx1            0.4080     0.1021   3.997 9.10e-05 ***\nx2            4.1642     0.3051  13.649  < 2e-16 ***\ngroupGp2      6.9809     2.0704   3.372 0.000901 ***\ngroupGp3      6.3161     1.7076   3.699 0.000282 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.49 on 195 degrees of freedom\nMultiple R-squared:  0.5385,    Adjusted R-squared:  0.5291 \nF-statistic: 56.89 on 4 and 195 DF,  p-value: < 2.2e-16\n\n\n\n\nDo Standardization Right\nInstall stdmod and load it:\n\nlibrary(stdmod)\n\nIf we want to standardize all variables except for categorical variables, if any, we just pass the output to std_selected_boot(), and set to_standardize to ~ .. The right-hand side of ~ denotes the variables to be standardized. If set to ., then all numeric variables, including the outcome variable (y), will be standardized.2\nBut this is not just about the coefficient. There is one issue with standardization: confidence intervals.\n\n\n\n\n\n\nBeware of the t-based SE and CI\n\n\n\nIf a variable is standardized, the usual t-based standard errors and confidence intervals of the coefficients that involve it may be biased.3\n\n\nThis is because (a) they do not take into account the sampling variation of the standard deviations used in standardization (Yuan & Chan, 2011), and (b) the coefficients with standardization, the “betas”, are not normally distributed (though may be close to). Many statistical programs do not report the confidence intervals for “betas,” for a good reason.\nThis is why std_selected_boot() enables nonparametric bootstrapping percentile confidence intervals by default, just in case the bias is large.\nTo have stable and reproducible confidence intervals, call set.seed() before calling std_selected_boot() and set nboot to the desired number of bootstrap samples (at least 2000 but 5000 or more is recommended):\n\nset.seed(870516)\nlm_out_std <- std_selected_boot(lm_out,\n                                to_standardize = ~ .,\n                                nboot = 5000)\n\nWe can call summary() as usual to print the results:\n\nsummary(lm_out_std)\n\n\nCall to std_selected_boot():\nstd_selected_boot(lm_out = lm_out, to_standardize = ~., nboot = 5000)\n\nSelected variable(s) are centered by mean and/or scaled by SD\n- Variable(s) centered: y x1 x2 group\n- Variable(s) scaled: y x1 x2 group\n\n      centered_by scaled_by                            Note\ny        40.36635 15.286176 Standardized (mean = 0, SD = 1)\nx1        9.41020  7.342158 Standardized (mean = 0, SD = 1)\nx2        5.35520  2.454193 Standardized (mean = 0, SD = 1)\ngroup          NA        NA Nonnumeric                     \n\nNote:\n- Categorical variables will not be centered or scaled even if\n  requested.\n- Nonparametric bootstrapping 95% confidence intervals computed.\n- The number of bootstrap samples is 5000.\n\nCall:\nlm(formula = y ~ x1 + x2 + group, data = dat_mod)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.79695 -0.46875  0.02324  0.43155  1.65619 \n\nCoefficients:\n            Estimate CI Lower CI Upper Std. Error t value Pr(>|t|)    \n(Intercept) -0.28825 -0.42260 -0.16397    0.08545  -3.374 0.000895 ***\nx1           0.19598  0.10064  0.28696    0.04903   3.997  9.1e-05 ***\nx2           0.66856  0.58901  0.73888    0.04898  13.649  < 2e-16 ***\ngroupGp2     0.45668  0.22507  0.68592    0.13544   3.372 0.000901 ***\ngroupGp3     0.41319  0.20033  0.63209    0.11171   3.699 0.000282 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6863 on 195 degrees of freedom\nMultiple R-squared:  0.5385,    Adjusted R-squared:  0.5291 \nF-statistic: 56.89 on 4 and 195 DF,  p-value: < 2.2e-16\n\nNote:\n- Estimates and their statistics are based on the data after\n  mean-centering, scaling, or standardization.\n- [CI Lower, CI Upper] are bootstrap percentile confidence intervals.\n- Std. Error are not bootstrap SEs.\n\n\n\n\n\nThe output has one additional section:\n\nVariables that are standardized4, and variables that are not transformed.\n\nThe other sections are similar to those of a usual multiple regression. Note that the column Estimate is intentionally not labelled as Beta because it is possible that only some variables are standardized. Labelling it as Beta, though common, is misleading.\n\n\nInterpret The Output\nIn the regression output, because y, x1, and x2 are standardized, they are the usual “betas.” However, group is not standardized and so groupGp2 and groupGp3 still take only two possible values, 0 and 1, and so can still be interpreted as usual.\nFor example, the coefficient of groupGp2 is 0.457. That is, compared to Gp1, the reference group, the predicted value of y is 0.457 SD (of y) higher in Gp2.\nOn the other hand, the coefficient of groupGp3 is 0.413. That is, compared to Gp1, the predicted value of y is 0.413 SD (of y) higher in Gp3.\nIn short, the coefficients of all dummy variables can be interpreted as usual, though the difference in the outcome variable (dependent variable) is in SD of this variable if it is standardized."
  },
  {
    "objectID": "posts/std_dummy/index.html#standardize-all-variables",
    "href": "posts/std_dummy/index.html#standardize-all-variables",
    "title": "Standardize Variables Except For Dummy Variables, Using std_selected_boot()",
    "section": "Standardize All Variables",
    "text": "Standardize All Variables\nAssume that we standardize all variables, including the dummy variables, as in some statistical program. To simulate this, I manually create the dummy variables, standardize them, and do regression:\n\n\nShow the code\ndat$groupGp2 <- 0\ndat$groupGp3 <- 0\ndat$groupGp2 <- ifelse(dat$group == \"Gp2\", 1, 0)\ndat$groupGp3 <- ifelse(dat$group == \"Gp3\", 1, 0)\ndat_std <- dat[, c(\"x1\", \"x2\", \"groupGp2\", \"groupGp3\", \"y\")]\ndat_std <- as.data.frame(scale(dat_std))\nhead(dat_std)\n\n\n           x1          x2   groupGp2   groupGp3          y\n1  0.07624462  1.01247114 -0.5220306 -0.9206479  0.3024726\n2 -0.94797747 -0.09991063 -0.5220306 -0.9206479  0.4516270\n3 -0.12396901  1.43216096  1.9060186 -0.9206479  1.2844056\n4 -0.72324789  0.09974764 -0.5220306 -0.9206479 -0.2169509\n5 -0.17027692  0.74354368 -0.5220306  1.0807606  0.3364903\n6 -0.93708149  1.56255004 -0.5220306 -0.9206479  0.9383413\n\n\nShow the code\npsych::describe(dat_std, range = FALSE, skew = FALSE)\n\n\n         vars   n mean sd   se\nx1          1 200    0  1 0.07\nx2          2 200    0  1 0.07\ngroupGp2    3 200    0  1 0.07\ngroupGp3    4 200    0  1 0.07\ny           5 200    0  1 0.07\n\n\nShow the code\nlm_out_std_wrong <- lm(y ~ x1 + x2 + groupGp2 + groupGp3, dat_std)\n\n\nThe following results are what found in common statistical programs that standardize all variables, including the dummy variables, to yield the “betas”:\n\n\nShow the code\nprintCoefmat(summary(lm_out_std_wrong)$coefficients,\n             zap.ind = 1:4,\n             P.values = TRUE)\n\n\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.000000   0.048525  0.0000  1.00000    \nx1          0.195980   0.049034  3.9968    9e-05 ***\nx2          0.668560   0.048983 13.6488  < 2e-16 ***\ngroupGp2    0.188080   0.055783  3.3717  0.00090 ***\ngroupGp3    0.206450   0.055817  3.6987  0.00028 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nLet us compare the results.\n\np-values\nThe p-values are the same, which is expected:\n\n\nShow the code\ncoef_std <- summary(lm_out_std)$coefficients\ncoef_std_wrong <- summary(lm_out_std_wrong)$coefficients\nround(cbind(`p-value (Std Selected)` = coef_std[, \"Pr(>|t|)\"],\n            `p-value (Std All)` = coef_std_wrong[, \"Pr(>|t|)\"]), 3)\n\n\n            p-value (Std Selected) p-value (Std All)\n(Intercept)                  0.001             1.000\nx1                           0.000             0.000\nx2                           0.000             0.000\ngroupGp2                     0.001             0.001\ngroupGp3                     0.000             0.000\n\n\n\n\nCoefficient Estimates\nHowever, the coefficients for the dummy variables are different:\n\n\nShow the code\nround(cbind(`Estimate (Std Selected)` = coef_std[, \"Estimate\"],\n            `Estimate (Std All)` = coef_std_wrong[, \"Estimate\"]), 3)\n\n\n            Estimate (Std Selected) Estimate (Std All)\n(Intercept)                  -0.288              0.000\nx1                            0.196              0.196\nx2                            0.669              0.669\ngroupGp2                      0.457              0.188\ngroupGp3                      0.413              0.206\n\n\nThe coefficients of groupGp2 and groupGp3, when they are standardized, are not meaningful. After standardization, they are no longer either 0 or 1.\nFor example, the so-called “beta” of groupGp2, 0.188, is not the difference between Gp2 and Gp1 on standardized y. It is the increase in standardized y when groupGp2 “increases by one SD”, which is meaningless.\nWhat’s worse, the ranking of the coefficients changed. If we interpret the coefficients with dummy variables not standardized, the Gp2 vs. Gp1 difference is larger than the Gp3 vs. Gp1 difference (0.457 vs 0.413).\nHowever, if we interpret the coefficients with dummy variables standardized, we would have a different conclusion: the Gp2 vs. Gp1 difference is smaller than the Gp3 vs. Gp1 difference (0.188 vs 0.206), though only slightly.\nThe reason is, there are more cases in Gp3 than in Gp2. The differences in the SDs of the dummy variables are large enough to reverse the ranking:\n\ntable(dat$group)\n\n\nGp1 Gp2 Gp3 \n 65  43  92 \n\nsd(dat$groupGp2)\n\n[1] 0.4118533\n\nsd(dat$groupGp3)\n\n[1] 0.4996481\n\n\nI created the data to demonstrate that this reversal in ranking is possible. This may not happen in real data. However, having groups with different numbers of cases is probably the norm rather than the exception in real data.5"
  },
  {
    "objectID": "posts/std_dummy/index.html#use-t-statistics-confidence-intervals",
    "href": "posts/std_dummy/index.html#use-t-statistics-confidence-intervals",
    "title": "Standardize Variables Except For Dummy Variables, Using std_selected_boot()",
    "section": "Use t Statistics Confidence Intervals",
    "text": "Use t Statistics Confidence Intervals\nSome programs gives confidence intervals of “betas” using t statistics. That is, standardize variables, do regression, and form the confidence intervals, as if the standardized variables were the original data.\nLet us compare the bootstrap confidence intervals with the usual OLS confidence intervals based on the t-statistics.\nFor the output of std_selected_boot(), we can request the t-based confidence intervals by setting type to \"lm\" when calling confint(), the function commonly used to form confidence intervals:\n\n# OLS confidence intervals\nround(confint(lm_out_std, type = \"lm\"), 3)\n\n             2.5 % 97.5 %\n(Intercept) -0.457 -0.120\nx1           0.099  0.293\nx2           0.572  0.765\ngroupGp2     0.190  0.724\ngroupGp3     0.193  0.634\n\n\nWithout setting type, the nonparametric bootstrap confidence intervals are returned (if available):\n\n# Bootstrap Confidence Intervals\nround(confint(lm_out_std), 3)\n\n             2.5 % 97.5 %\n(Intercept) -0.423 -0.164\nx1           0.101  0.287\nx2           0.589  0.739\ngroupGp2     0.225  0.686\ngroupGp3     0.200  0.632\n\n\nAs shown above, the confidence intervals of x1 by the two methods are close to each other. However, the bootstrap confidence interval of x2 is narrower than the t-based confidence interval.\nWe can compare the widths of the confidence intervals by examining their ratios (ratio = CI_Width_t / CI_Width_boot):\n\n\nShow the code\nci_t <- confint(lm_out_std, type = \"lm\")\nci_b <- confint(lm_out_std)\nwidth_t <- ci_t[, 2] - ci_t[, 1]\nwidth_b <- ci_b[, 2] - ci_b[, 1]\nci_compare <- data.frame(CI_Width_t = width_t,\n                         CI_Width_boot = width_b)\nci_compare$ratio <- ci_compare[, 1] / ci_compare[, 2]\nround(ci_compare, 3)\n\n\n            CI_Width_t CI_Width_boot ratio\n(Intercept)      0.337         0.259 1.303\nx1               0.193         0.186 1.038\nx2               0.193         0.150 1.289\ngroupGp2         0.534         0.461 1.159\ngroupGp3         0.441         0.432 1.021\n\n\n\n\n\nThe widths of the confidence intervals are nearly identical for x1. However, for x2, the t-based confidence interval is nearly 28.9% wider than the bootstrap confidence interval.\nFor groupGp2, even though dummy variable is correctly not standardized, it can still be affected because the outcome variable, y, is standardized. Its t-based confidence interval is also wider than the bootstrap confidence interval by about 15.9%."
  },
  {
    "objectID": "posts/std_mod/index.html",
    "href": "posts/std_mod/index.html",
    "title": "Proper Standardization in Moderated Regression Using std_selected_boot()",
    "section": "",
    "text": "This post shows one simple way to get correctly standardized regression coefficients in multiple linear regression with a moderator, with appropriate confidence intervals, using std_selected_boot() from the stdmod package.\n(Note: This post and some related posts, e.g., this one on categorical variables, have sections that are identical or highly similar. This is intentional, to make each article self-contained. Readers do not need to refer to other articles to understand the points.)"
  },
  {
    "objectID": "posts/std_mod/index.html#example",
    "href": "posts/std_mod/index.html#example",
    "title": "Proper Standardization in Moderated Regression Using std_selected_boot()",
    "section": "Example",
    "text": "Example\n\nDo Regression As Usual\nSuppose this is the data set:\n\n\nShow the code for generating data\nset.seed(342532)\nn <- 200\ncontrol1 <- rnorm(n, 5, 1)\ncontrol2 <- rnorm(n, 20, 5)\nx <- rnorm(n, 10, 4)\nw <- rnorm(n, 10, 2)\ny <- 10 + 1 * control1 + 2 * control2 +\n     (3 + 1 * (w - 10)) * x + rnorm(n, 0, 30)\ndat <- data.frame(control1, control2, x, w, y)\nwrite.csv(dat, \"dat.csv\")\n\n\nThis is the data file.\n\nhead(dat)\n\n  control1 control2         x         w         y\n1 5.025877 22.32080  9.521779 11.049711 131.15905\n2 6.200119 17.04644 11.912240 10.860813 102.01347\n3 4.992346 27.28403  9.845949 10.515219  77.86161\n4 4.979231 12.85510  6.512449 13.635488  89.54024\n5 7.020257 15.48984  5.734798  9.846383  66.94897\n6 3.969682 24.61428  3.329810 10.221247  38.20317\n\n\nThe variable x is the predictor, w the moderator. The dataset also has two control variables, control1 and control2. The outcome variable is y.\nThis is the regression model.\n\nlm_out <- lm(y ~ control1 + control2 + x*w, dat)\nsummary(lm_out)\n\n\nCall:\nlm(formula = y ~ control1 + control2 + x * w, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-86.895 -18.374  -0.556  19.255  70.971 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  40.8781    29.5919   1.381 0.168746    \ncontrol1     -2.2750     2.1314  -1.067 0.287129    \ncontrol2      1.0053     0.3821   2.631 0.009191 ** \nx            -4.6922     2.3479  -1.998 0.047060 *  \nw             0.3635     2.4452   0.149 0.881986    \nx:w           0.7780     0.2287   3.402 0.000811 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28.34 on 194 degrees of freedom\nMultiple R-squared:  0.3587,    Adjusted R-squared:  0.3422 \nF-statistic:  21.7 on 5 and 194 DF,  p-value: < 2.2e-16\n\n\n\n\n\nThe moderation effect is significant.\n\nIf w increases by one unit, the effect of x increases by 0.778.\nIf w is equal to zero, the effect of x is -4.692, and significant.\n\n\n\nDo Standardization Right\nInstall stdmod and load it:\n\nlibrary(stdmod)\n\nIf we want to standardize all variables except for the product term, and compute the product term as the product of the standardized variables, we just pass the output to std_selected_boot(), and set to_standardize to ~ .. The right-hand side of ~ denotes the variables to be standardized. If set to ., then all numeric variables, including the outcome variable (y), will be standardized.2\nBut this is not just about the coefficient. There is one issue with standardization: confidence intervals.\n\n\n\n\n\n\nBeware of the t-based SE and CI\n\n\n\nIf a variable is standardized, the usual t-based standard errors and confidence intervals of the coefficients that involve it may be biased.3\n\n\nThis is because (a) they do not take into account the sampling variation of the standard deviations used in standardization (Yuan & Chan, 2011), and (b) the coefficients with standardization, the “betas”, are not normally distributed (though may be close to). Many statistical programs do not report the confidence intervals for “betas,” for a good reason.\nThe case of moderated regression is more complicated because, as shown in Cheung et al. (2022), the correctly standardized product term involves the standard deviations of three variables, not two.\nThis is why std_selected_boot() enables nonparametric bootstrapping percentile confidence intervals by default, just in case the bias is large.\nTo have stable and reproducible confidence intervals, call set.seed() before calling std_selected_boot() and set nboot to the desired number of bootstrap samples (at least 2000 but 5000 or more is recommended):\n\nset.seed(645321)\nlm_out_std <- std_selected_boot(lm_out,\n                                to_standardize = ~ .,\n                                nboot = 5000)\n\nCall summary() as usual:\n\nsummary(lm_out_std)\n\n\nCall to std_selected_boot():\nstd_selected_boot(lm_out = lm_out, to_standardize = ~., nboot = 5000)\n\nSelected variable(s) are centered by mean and/or scaled by SD\n- Variable(s) centered: y control1 control2 x w\n- Variable(s) scaled: y control1 control2 x w\n\n         centered_by  scaled_by                            Note\ny          83.477789 34.9370599 Standardized (mean = 0, SD = 1)\ncontrol1    5.060548  0.9562693 Standardized (mean = 0, SD = 1)\ncontrol2   19.625484  5.3780625 Standardized (mean = 0, SD = 1)\nx           9.844108  4.3166239 Standardized (mean = 0, SD = 1)\nw          10.132063  2.0474763 Standardized (mean = 0, SD = 1)\n\nNote:\n- Categorical variables will not be centered or scaled even if\n  requested.\n- Nonparametric bootstrapping 95% confidence intervals computed.\n- The number of bootstrap samples is 5000.\n\nCall:\nlm(formula = y ~ control1 + control2 + x * w, data = dat_mod)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.48720 -0.52591 -0.01591  0.55113  2.03139 \n\nCoefficients:\n             Estimate  CI Lower  CI Upper Std. Error t value Pr(>|t|)    \n(Intercept)  0.020131 -0.005886  0.052164   0.057654   0.349 0.727347    \ncontrol1    -0.062269 -0.152822  0.036064   0.058339  -1.067 0.287129    \ncontrol2     0.154750  0.045436  0.265029   0.058813   2.631 0.009191 ** \nx            0.394150  0.277605  0.502798   0.059581   6.615 3.52e-10 ***\nw            0.470113  0.358552  0.573546   0.057982   8.108 5.71e-14 ***\nx:w          0.196803  0.098112  0.289331   0.057843   3.402 0.000811 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.811 on 194 degrees of freedom\nMultiple R-squared:  0.3587,    Adjusted R-squared:  0.3422 \nF-statistic:  21.7 on 5 and 194 DF,  p-value: < 2.2e-16\n\nNote:\n- Estimates and their statistics are based on the data after\n  mean-centering, scaling, or standardization.\n- [CI Lower, CI Upper] are bootstrap percentile confidence intervals.\n- Std. Error are not bootstrap SEs.\n\n\n\n\n\nThe output has one additional section:\n\nVariables that are standardized4, and variables that are not transformed.\n\nAs shown in the first section, x and w are standardized. x:w is not on the list. It is because x:w is not standardized. It should not be, as explained above.\nThe other sections are similar to those for a usual multiple regression. Note that the column Estimate is intentionally not labelled as Beta because it is possible that only some variables are standardized. Labelling it as Beta, though common, is misleading.\n\n\nInterpret The Output\nThe coefficient of the product term x:w is 0.197. That is, for each one SD increase of w, the standardized effect of x on y ( the “beta” of x on y) increases by 0.197.\nMy collaborator and I proposed to call the moderation effect with x, w, and y standardized, x:w in the example, the standardized moderation effect (Cheung et al., 2022). When variables are standardized as proposed by Friedrich (1982), the coefficients can be interpreted as usual in moderated regression, with all variables on the standardized metric.\nThe coefficient of x is 0.394. This is the standardized effect (the “beta”) of x when w in this model is equal to zero. Because w is standardized, this is equivalent to say that this is the standardized effect of x when w is equal to its mean because the mean of a standardized variable is zero.\n\n\nConditional Effect\nThe function cond_effect_boot() from stdmod can be used to compute the conditional effects. Just pass the output of lm() or std_selected_boot() to it.\nWhen the variables are standardized, cond_effect_boot() can be used to compute the standardized conditional effects, with nonparametric bootstrap confidence intervals. Just set nboot to the desired number of bootstrap samples. To ensure that the same nboot bootstrap samples are drawn, set the seed to the number used in std_seleted_boot().\n\nset.seed(645321)\ncond_std <- cond_effect_boot(lm_out_std,\n                             x = \"x\",\n                             w = \"w\",\n                             nboot = 5000)\ncond_std\n\nThe effects of x on y, conditional on w:\n\n  Level      w x Effect CI Lower CI Upper  S.E.     t     p Sig\n   High  1.000    0.591    0.429    0.737 0.085 6.982 0.000 ***\n Medium  0.000    0.394    0.278    0.503 0.060 6.615 0.000 ***\n    Low -1.000    0.197    0.051    0.340 0.081 2.424 0.016 *  \n\n[CI Lower, CI Upper] shows the 95% nonparametric bootstrap confidence\ninterval(s) (based on 5000 bootstrap samples).\n\n\nThe regression model:\n\n    y ~ control1 + control2 + x * w\n\nInterpreting the levels of w:\n\n  Level      w % Below From Mean (in SD)\n   High  1.000   81.50              1.00\n Medium  0.000   51.00              0.00\n    Low -1.000   15.00             -1.00\n\n- % Below: The percent of cases equal to or less than a level.\n- From Mean (in SD): Distance of a level from the mean, in standard\n  deviation (+ve above, -ve below).\n\nNote:\n\n- The variable(s) y, x, w is/are standardized.\n- The conditional effects are the standardized effects of x on y.\n\n\n\n\n\nAs shown above, the effect of x is positive and significant when w ranges from one SD below mean to one SD above mean. The standardized effect of x on y when w is one SD above mean is 0.591."
  },
  {
    "objectID": "posts/std_mod/index.html#standardize-the-product-term",
    "href": "posts/std_mod/index.html#standardize-the-product-term",
    "title": "Proper Standardization in Moderated Regression Using std_selected_boot()",
    "section": "Standardize the Product Term",
    "text": "Standardize the Product Term\nAssume that we standardize all variables, including the product term, as in some statistical program. To simulate this, let’s manually create the product term, standardize all variables, including the product term, and do regression:\n\n\nShow the code\ndat$x_w <- dat$x * dat$w\ndat_std <- as.data.frame(scale(dat))\nhead(dat_std)\n\n\n     control1   control2             x           w          y        x_w\n1 -0.03625650  0.5011678 -0.0746715806  0.44818493  1.3647761  0.1360233\n2  1.19168346 -0.4795487  0.4791086834  0.35592570  0.5305451  0.6515372\n3 -0.07132155  1.4240348  0.0004264965  0.18713570 -0.1607514  0.1001691\n4 -0.08503578 -1.2588894 -0.7718205413  1.71109398  0.1735248 -0.2141245\n5  2.04932709 -0.7689844 -0.9519731607 -0.13952791 -0.4731027 -0.9039326\n6 -1.14075236  0.9276200 -1.5091188133  0.04355764 -1.2958909 -1.3825065\n\n\nShow the code\npsych::describe(dat_std, range = FALSE, skew = FALSE)\n\n\n         vars   n mean sd   se\ncontrol1    1 200    0  1 0.07\ncontrol2    2 200    0  1 0.07\nx           3 200    0  1 0.07\nw           4 200    0  1 0.07\ny           5 200    0  1 0.07\nx_w         6 200    0  1 0.07\n\n\nShow the code\nlm_out_std_wrong <- lm(y ~ control1 + control2 + x + w + x_w, dat_std)\n\n\nThe following results are what found in common statistical programs that standardize all variables, including the product term, to yield the “betas”:\n\n\nShow the code\nprintCoefmat(summary(lm_out_std_wrong)$coefficients,\n             zap.ind = 1:4,\n             P.values = TRUE)\n\n\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.000000   0.057350  0.0000  1.00000    \ncontrol1    -0.062270   0.058339 -1.0674  0.28713    \ncontrol2     0.154750   0.058813  2.6312  0.00919 ** \nx           -0.579740   0.290091 -1.9985  0.04706 *  \nw            0.021300   0.143300  0.1486  0.88199    \nx_w          1.043740   0.306771  3.4023  0.00081 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nLet us compare the results.\n\np-values\nSome p-values are different, which is expected:\n\n\nShow the code\ncoef_std <- summary(lm_out_std)$coefficients\ncoef_std_wrong <- summary(lm_out_std_wrong)$coefficients\nround(cbind(`p-value (Std Correct)` = coef_std[, \"Pr(>|t|)\"],\n            `p-value (Std All)` = coef_std_wrong[, \"Pr(>|t|)\"]), 5)\n\n\n            p-value (Std Correct) p-value (Std All)\n(Intercept)               0.72735           1.00000\ncontrol1                  0.28713           0.28713\ncontrol2                  0.00919           0.00919\nx                         0.00000           0.04706\nw                         0.00000           0.88199\nx:w                       0.00081           0.00081\n\n\nThe p-values of x:w are the same, which is expected (see Aiken & West, 1991).\nHowever, the p-values of x and w are very different. This is not just because they are effects of x and w conditional on other values. They actually are not conditional effects as we usually understood, explained in the next section.\n\n\nCoefficient Estimates\nLet us compare the coefficients:\n\n\nShow the code\nround(cbind(`Estimate (Std Correct)` = coef_std[, \"Estimate\"],\n            `Estimate (Std All)` = coef_std_wrong[, \"Estimate\"]), 3)\n\n\n            Estimate (Std Correct) Estimate (Std All)\n(Intercept)                  0.020              0.000\ncontrol1                    -0.062             -0.062\ncontrol2                     0.155              0.155\nx                            0.394             -0.580\nw                            0.470              0.021\nx:w                          0.197              1.044\n\n\n\n\n\nThe coefficients of x, w, and x:w have very different values between the correct standardized solution and the standardized solution with product term standardized.\nTwo points of note for the second column, the solution with the product term itself standardized:\n\nThe coefficient of x:w , when it is standardized (called x_w), is not a product term. The value 1.044 is not the increase of effect of x when w increases by one SD, nor by one unit.\nThe coefficient of x is not the usual conditional effect. Mathematically, it is the standardized effect of x when w is equal to “some value”. However, what is this value? This is not easy to determine because x_w is not a product term. The coefficient of w has the same problem.\n\n\n\n\n\n\n\nStandardizing the Product Term Make Interpretation Difficult\n\n\n\n\nThe coefficient of the product term is no longer the moderation effect (except in some very special cases).\nThe coefficients of the component terms, the focal variable and the moderator, are no longer the conditional effects of values easily deducible from the results."
  },
  {
    "objectID": "posts/std_mod/index.html#use-t-statistics-confidence-intervals",
    "href": "posts/std_mod/index.html#use-t-statistics-confidence-intervals",
    "title": "Proper Standardization in Moderated Regression Using std_selected_boot()",
    "section": "Use t Statistics Confidence Intervals",
    "text": "Use t Statistics Confidence Intervals\nSome programs gives confidence intervals of “betas” using t statistics. That is, standardize variables, do regression, and form the confidence intervals, as if the standardized variables were the original data.\nLet us compare the bootstrap confidence intervals with the usual OLS confidence intervals based on the t-statistics.\nFor the output of std_selected_boot(), we can request the t-based confidence intervals by setting type to \"lm\" when calling confint(), the function commonly used to form confidence intervals:\n\n# OLS confidence intervals\nround(confint(lm_out_std, type = \"lm\"), 3)\n\n             2.5 % 97.5 %\n(Intercept) -0.094  0.134\ncontrol1    -0.177  0.053\ncontrol2     0.039  0.271\nx            0.277  0.512\nw            0.356  0.584\nx:w          0.083  0.311\n\n\nWithout setting type, the nonparametric bootstrap confidence intervals are returned (if available):\n\n# Bootstrap Confidence Intervals\nround(confint(lm_out_std), 3)\n\n             2.5 % 97.5 %\n(Intercept) -0.006  0.052\ncontrol1    -0.153  0.036\ncontrol2     0.045  0.265\nx            0.278  0.503\nw            0.359  0.574\nx:w          0.098  0.289\n\n\nAs shown above, the confidence intervals of x, w, and x:w by the two methods are close to each other. However, the bootstrap confidence intervals tend to be narrower than the t-based confidence intervals.\nWe can compare the widths of the confidence intervals by examining their ratios (ratio = CI_Width_t / CI_Width_boot):\n\n\nShow the code\nci_t <- confint(lm_out_std, type = \"lm\")\nci_b <- confint(lm_out_std)\nwidth_t <- ci_t[, 2] - ci_t[, 1]\nwidth_b <- ci_b[, 2] - ci_b[, 1]\nci_compare <- data.frame(CI_Width_t = width_t,\n                         CI_Width_boot = width_b)\nci_compare$ratio <- ci_compare[, 1] / ci_compare[, 2]\nround(ci_compare, 3)\n\n\n            CI_Width_t CI_Width_boot ratio\n(Intercept)      0.227         0.058 3.918\ncontrol1         0.230         0.189 1.218\ncontrol2         0.232         0.220 1.056\nx                0.235         0.225 1.044\nw                0.229         0.215 1.064\nx:w              0.228         0.191 1.193\n\n\n\n\n\nThe t-based confidence interval of the product term, the standardized moderation effect, is nearly 19.3% wider than the bootstrap confidence interval."
  },
  {
    "objectID": "posts/the_first_post/index.html",
    "href": "posts/the_first_post/index.html",
    "title": "Reviving an Old Blog",
    "section": "",
    "text": "This is a “revival” of an old blog of mine on psychological research and methodology (hence the name Blogonresearch). Just anything on research and methodology that are useful to me, and maybe also useful to others.\nEdit 2023-01-28: This is actually the “second revival” as I switched from Hugo to Quarto in early 2023."
  }
]