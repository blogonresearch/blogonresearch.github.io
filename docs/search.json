[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a “revival” of an old blog of mine on psychological research and methodology (hence the name Blogonresearch). Just anything on research and methodology that are useful to me, and maybe also useful to others.\n– Shu Fai Cheung\nP.S.: This is actually the “second revival” as I switched from Hugo to Quarto in early 2023."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogonresearch",
    "section": "",
    "text": "Bootstrap Confidence Intervals for Standardized Solution in lavaan\n\n\n\n\n\n\n\nR\n\n\nlavaan\n\n\nbootstrapping\n\n\nconfidence-intervals\n\n\nsemhelpinghands\n\n\nstandardized\n\n\n\n\n\n\n\n\n\n\n\nWednesday, September 28, 2022\n\n\nShu Fai Cheung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow Options Set by lavaan\n\n\n\n\n\n\n\nR\n\n\nlavaan\n\n\nsemhelpinghands\n\n\n\n\n\n\n\n\n\n\n\nMonday, September 26, 2022\n\n\nShu Fai Cheung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Own Style in R\n\n\n\n\n\n\n\nR\n\n\ncode-style\n\n\n\n\n\n\n\n\n\n\n\nSaturday, September 24, 2022\n\n\nShu Fai Cheung\n\n\n\n\n\n\n  \n\n\n\n\nCustomize R GUI For Windows\n\n\n\n\n\n\n\nR\n\n\ngui\n\n\n\n\n\n\n\n\n\n\n\nTuesday, September 20, 2022\n\n\nShu Fai Cheung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne Function or Many Functions\n\n\n\n\n\n\n\nR\n\n\ngui\n\n\n\n\n\n\n\n\n\n\n\nWednesday, September 14, 2022\n\n\nShu Fai Cheung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Moderation Effects With ggplot2\n\n\n\n\n\n\n\nR\n\n\nmoderation\n\n\n\n\n\n\n\n\n\n\n\nSunday, September 11, 2022\n\n\nShu Fai Cheung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReviving an Old Blog\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nSunday, September 11, 2022\n\n\nShu Fai Cheung\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bootstrap_confidence_intervals_for_standardized_solution_in_lavaan/index.html",
    "href": "posts/bootstrap_confidence_intervals_for_standardized_solution_in_lavaan/index.html",
    "title": "Bootstrap Confidence Intervals for Standardized Solution in lavaan",
    "section": "",
    "text": "lavaan supports bootstrap confidence intervals for free and user-defined parameters. This is useful especially for parameter estimates that may not be approximately normally distributed unless the sample size is very large.\nHowever, it is known, though not well-known enough in my opinion, that, even if bootstrap confidence intervals are requested, the confidence intervals reported in the standardized solution are not bootstrap confidence intervals as in tools like PROCESS for standardized effects like standardized indirect effects, but are symmetric delta-method confidence intervals based on the bootstrap sampling variance-covariance matrix.\nLet’s use a sample dataset for illustration:\n\n# Create the data\nset.seed(860541)\nn <- 100\nx <- rnorm(n)\nm <- .4 * x + rnorm(n, 0, sqrt(1 - .3^2))\ny <- .4 * m + rnorm(n, 0, sqrt(1 - .4^2))\ndat <- data.frame(x = 10 * x, m = 2 * m, y = 3 * y)\n\nWe specify a simple regression model:\n\nlibrary(lavaan)\n\nThis is lavaan 0.6-13\nlavaan is FREE software! Please report any bugs.\n\nmod <-\n\"\nm ~ a * x\ny ~ b * m + cp * x\nab := a * b\n\"\n\n… and fit it with bootstrap confidence intervals (2023-01-28: Code and results updated for lavaan 0.6-13, iseed is used instead of set.seed()):\n\nfit <- sem(mod, data = dat, fixed.x = FALSE,\n           se = \"boot\", bootstrap = 2000,\n           iseed = 8970)\n\n\n\n\nLet’s focus on the confidence intervals of the indirect effect:\n\nest <- parameterEstimates(fit)\nstd <- standardizedSolution(fit)\n# Unstandardized\nest[7, ]\n\n  lhs op rhs label   est    se     z pvalue ci.lower ci.upper\n7  ab := a*b    ab 0.025 0.015 1.686  0.092    0.001    0.059\n\n# Standardized\nstd[7, ]\n\n  lhs op rhs label est.std    se     z pvalue ci.lower ci.upper\n7  ab := a*b    ab   0.088 0.049 1.774  0.076   -0.009    0.185\n\n\nThey lead to different conclusions.\nAs shown below, the confidence interval of the unstandardized indirect effect is percentile confidence interval that is asymmetric, as expected:\n\nest[7, c(\"ci.lower\", \"ci.upper\")] - est[7, \"est\"]\n\n     ci.lower   ci.upper\n7 -0.02364024 0.03392409\n\n\nHowever, the confidence interval of the standardized indirect effect is symmetric:\n\nstd[7, c(\"ci.lower\", \"ci.upper\")] - std[7, \"est.std\"]\n\n     ci.lower   ci.upper\n7 -0.09699904 0.09699904\n\n\n\n\n\nThis behavior has been discussed in the Google group forlavaan and so is known, but not “well-known” because I met many users who were not aware of this, especially when they use bootstrapping to get the confidence intervals for indirect effects but found that the confidence intervals of unstandardized and standardized indirect effect led to different conclusions, as in the example above.\nA solution already exists in lavaan. Users can use bootstrapLavaan() and get the bootstrap confidence intervals for many results, including the output of standardized solution.\nWe first define a function to extract the standardized indirect effect:\n\nfct <- function(fit) {\n    lavaan::standardizedSolution(fit)[7, \"est.std\"]\n  }\n\nWe then update the fit object to disable standard error because we only need the point estimates and then call bootstrapLavaan():\n\nfit0 <- update(fit, se = \"none\")\nfit_boot <- bootstrapLavaan(fit0, R = 2000, FUN = fct, iseed = 8970)\n\n\n\n\nThe percentile confidence interval can then be formed by quantile().\n(Note that lavaan() does not use quantile() but use the approach by boot.ci(). The resulting interval can be slightly different from that by quantile().)\n\nquantile(fit_boot[, 1], c(.025, .975))\n\n       2.5%       97.5% \n0.004372947 0.196203435 \n\n\nHowever, this is inconvenient because we need to write custom function, and bootstrapping was done twice unless we store both the unstandardized and standardized solutions in the custom function used when calling bootstrapLavaan().\nI wrote the function standardizedSolution_boot_ci(), available in the package semhelpinghands, for this particular case that I sometimes encounter:\n\nA model is already fitted with se = \"boot\" and so bootstrap confidence intervals are already available for the unstandardized estimates.\nI want to get the bootstrap confidence intervals for the standardized solution without doing the bootstrapping again.\n\nThis would be useful to me because some of my projects involve large samples with missing data. and bootstrapping takes appreciable time even with parallelization.\nThis is how to use this function:\n\nlibrary(semhelpinghands)\nstd_boot <- standardizedSolution_boot_ci(fit)\n# -c(9, 10) is used to remove the delta-method CIs from\n# the printout\nstd_boot[, -c(9, 10)]\n\n  lhs op rhs label est.std    se      z pvalue boot.ci.lower boot.ci.upper\n1   m  ~   x     a   0.232 0.105  2.213  0.027         0.015         0.425\n2   y  ~   m     b   0.379 0.083  4.541  0.000         0.204         0.541\n3   y  ~   x    cp   0.103 0.092  1.117  0.264        -0.079         0.281\n4   m ~~   m         0.946 0.048 19.527  0.000         0.819         0.999\n5   y ~~   y         0.828 0.073 11.403  0.000         0.660         0.940\n6   x ~~   x         1.000 0.000     NA     NA            NA            NA\n7  ab := a*b    ab   0.088 0.049  1.774  0.076         0.004         0.196\n\n\nThe boot.ci intervals are “true” bootstrap confidence intervals, formed from the bootstrap estimates. The bootstrap confidence interval for the standardized indirect effect ([0.004, 0.196]) and that for the unstandardized indirect effect ([0.001, 0.059]) now lead to the same conclusion.\nstandardizedSolution_boot_ci() works like standardizedSolution(), but extracts the stored bootstrap estimates, get the standardized solution from each set of estimates, and use them to form the bootstrap confidence intervals for the standardized solution.\nBy default, the bootstrap standardized solution is also stored in the attribute boot_est_std. They can be extracted to examine the distribution. For example, the bootstrap standardized indirect effects can be extracted and plotted:\n\nstd_boot_est <- attr(std_boot, \"boot_est_std\")\nstd_indirect_boot_est <- std_boot_est[, 7]\nhist(std_indirect_boot_est)\n\n\n\nqqnorm(std_indirect_boot_est)\nqqline(std_indirect_boot_est)\n\n\n\n\nThis function is simple to use, at least for me. No need to write custom function, and no need to do bootstrapping twice. In most cases, I don’t even need to specify any additional arguments.\nMore about this function can be found in the vignette for standardizedSolution_boot_ci().\nIf any bug in standardizedSolution_boot_ci() was found, I would appreciate submitting it as a GitHub issue."
  },
  {
    "objectID": "posts/customizes_r_gui_for_windows/index.html",
    "href": "posts/customizes_r_gui_for_windows/index.html",
    "title": "Customize R GUI For Windows",
    "section": "",
    "text": "Many people use RStudio, and some even mistaken RStudio as R. I mainly work on Windows machines and I did try RStudio for a while a few years ago. However, I switched back to default R GUI for Windows that comes with R for Windows and used it along with light-weight code editors, for personal reasons. RStudio is good, but has many features that I don’t need. Although I now use VS Code as my main IDE for R, I still use R GUI for Windows a lot. It is light-weight, came with base installation, and is customizable. Simple but good enough for some tasks.\n\n\n\nR Default GUI\n\n\nThe console above is not the default one. I like dark theme and single-document interface (SDI). I keep only a limited numbers of windows on my desktop and I never group windows in the taskbar. SDI is much more efficient for me to locate the window I need.\nThe configuration can be set in Edit->Preferences:\n\n\n\nRgui configuration\n\n\nThe configuration is saved in the Rconsole file in the folder etc in R’s installation folder. Whenever I upgrade to a new version of R, I simply copy this file to the same folder in the new installation of R to have my preferred configuration. No need to set the configuration again. (I’ve just found that I haven’t changes this configuration for over five years!)\nAnother feature I like is customizing the menu bar. I use devtools a lot, and would love to call them from the pulldown menu … well, not really. I rarely use the mouse to access the pulldown menu. I use the keyboard most of the time.\n\n\n\nR GUI pulldown menu\n\n\nTo add a menu, we can use a script and two functions.\nThe following function add a menu called devtools:\n\nutils::winMenuAdd(\"devtools\")\n\nAfter a menu is added, items can be added by utils::winMenuAddItem(). For example, the following call add check to the menu devtools. If selected, the call devtools::check() will be executed:\n\nutils::winMenuAddItem(\"devtools\",\n                      \"check\",\n                      \"devtools::check()\")\n\nAs the screenshot above showed, most of the items I added to devtools are those functions (menu items) in RStudio (as far as I recalled … as I haven’t used RStudio to develop package for a long time).\nMost common tasks that can be done through R code can be converted to a menu item. For example, I don’t like using the pulldown menu to change working directory. I can quickly copy the path to a folder using keyboard only (alt-D and then control-C in Explorer). Therefore, I have the following menu item added to the R GUI:\n\nutils::winMenuAddItem(\"Utils\",\n  \"setwd from clipboard\",\n  \"setwd(readClipboard());getwd()\")\n\nsetwd(readClipboard()) changes the working directory to the folder I copied to clipboard. getwd() is added just to confirm the change.\nI sometimes open several instances of R GUI. To differentiate them, I use different color schemes:\n\n\n\nFour instances of R GUI\n\n\nThis can be done inside R GUI too:\n\nutils::winMenuAddItem(\"Style\",\n  \"Lime Green and Light Golden\",\n  \"utils::loadRconsole('D:/My_Settings/Rconsole_lime_green_and_light_golden')\")\n\nutils::loadRconsole() is used to load the R GUI configuration stored in a file. This file can be created by saving the settings in the R GUI configuration dialog above to a file. In the above example, the settings are stored in the file Rconsole_lime_green_and_light_golden. I have four such files, storing distinct color schemes.\nLike Rconsole for the color scheme and SDI, I would like to have custom menus when I start R GUI. This can be done by adding code like the following to the Rprofile.site file in the etc folder:\n\nif ((Sys.getenv(\"RS_LOCAL_PEER\") == \"\") && (.Platform$GUI == \"Rgui\")) {\n    tryCatch(source(\"D:/My_Settings/add_style_menu.R\"), error = function(e) e)\n  }\n\nI wrote this a long long time ago and I cannot recall their purposes. I believe Sys.getenv(\"RS_LOCAL_PEER\") == \"\" is used to check whether a session is launched in RStudio. .Platform$GUI == \"Rgui\" is used to check whether an R session is launched by the default R GUI. I still occasionally use RStudio and the menus I created should not be added if an R session is launched inside RStudio. I used tryCatch() just in case there are issues that I overlooked.\nSo, whenever I install a new copy or version of R, I just copy and paste the customized Rconsole and Rprofile.site files to etc. I can then happily have my preferred environment. Having the code for menu groups stored in other files allow me to update them without editing Rprofile.site.\nHope you find these tips useful … if you are R GUI users like me. :)"
  },
  {
    "objectID": "posts/my_own_style_in_r/index.html",
    "href": "posts/my_own_style_in_r/index.html",
    "title": "My Own Style in R",
    "section": "",
    "text": "Although I have written programs since I were a high school student, when computer monitor could only display one color, I have no formal training in programming, and I rarely worked with others in developing a solution until recently. The problem: I did not write with a consistent and professional style. I am pretty sure that my code will look “ugly” to professional programmers.\nThat said, I do have a loose style, one that suits my own situation:\n\nI work on small screen frequently, sometimes even on my mobile phone. I wrote the documentation of some packages on my mobile phone, and even this post was largely drafted on my phone.\nMy main tasks are research and teaching, among other tasks. I can easily forget some style rules I set for coding. I need something simple and easy to remember.\nIdeally, code should be easy, or at least not too difficult, to comprehend by future me with minimal comments. What looks nature to me myself is of the top priority.\n\nSo, this is my style, with me as the main user and reader:\n\nFor code, I use a line width of 60 to 70. For documentation, I am more aggressive and use a line width of 40.\nFor the same reason, I use two spaces for indentation. A four-space indentation is too “expensive” to me.\n\n\ntmpfct <- function(x) {\n    x^2\n  }\n\n\nI like using double-click to select a name. This does not work if periods are used inside a name. Therefore, I no longer use periods in a name, except for S3 methods. If necessary, I use underscores.\n\n\nthis_is_long_name <- 1\n\n\nI found it difficult to remember the case I used for a name. So, I stick to lowercase letters unless I am very certain that I can remember that I used uppercase letters.\n\nI don’t like camel case. It is OK for language that is not case sensitive, like Visual Basic and SPSS syntax commands, but is inconvenient for case sensitive languages like R.\n\n# I don't like camel case.\nthisIsNotWhatIDo <- 1\n# I prefer this:\nthis_is_what_i_do <- 1\n\n\nLong function names are acceptable with me. With autocompletion in many IDEs, it is not important to use short names. Being easy to remember part of a name is important. An abbreviation is not easy to remember unless it is commonly used (e.g., SD).\n\n\n# This is easy to remember\nfactor_loadings()\n# These variants are not\nfload()\nfacload()\nfacload()\nfl()\n\n\nI wrote stuff in Python occasionally. I like the Python style indentation, which is easy to read. So I use that style for my R code too.\n\n\nfor (j in 1:10) {\n    # Do something\n  }\nif (x == 1) {\n    # Do something\n  } else {\n    # Do something else\n  }\n\n\nI never, ever, use any automatic stylers to reformat code. They make changes that are tracked by Git but have nothing to do with the content. I may use them, but only when finalizing the code.\nThis is also why I care little about word wrap. Irregular line widths are acceptable for me.\nExtra whitespace are OK with me. Readability is the main goal.\n\n\n# I may do this:\nx  <-   1\ny0 <- 100\n\n\nI use double quotes for string literals. I have to use two keys … but the habit is too difficult to break that I don’t bother changing it.\nThe last “rule”: I can break any of the rules, as long as the code is readable without the need to know any rules.\n\nI also have a GitHub repo for my personal style, in case I forgot the rules:\nhttps://github.com/sfcheung/rstylesf\nSo, please pardon me if you find my code for packages at odd with professional style. I myself is the main reader and maintainer of the packages. What work for me matters."
  },
  {
    "objectID": "posts/one_function_or_many_functions/index.html",
    "href": "posts/one_function_or_many_functions/index.html",
    "title": "One Function or Many Functions",
    "section": "",
    "text": "I am thinking about the differences between R and SPSS in doing analysis: one function for one analysis, and several functions for one analysis.\nThough not always the case, in R, it is common to do an analysis using several functions. One of them is the “main” function that do the main analysis. Other functions are used to extract information or compute other statistics.\nFor example, to do multiple regression, this is what we may do:\n\n# Create a Test Dataset\nset.seed(586045)\nn <- 100\ndat <- data.frame(x1 = rnorm(n, 5, 1),\n                  x2 = rnorm(n, 10, 2))\ndat$y <- 2 * dat$x1 + 1 * dat$x2 + rnorm(n, 0, 15)\n# Do regression\nlm_out <- lm(y ~ x1 + x2, dat)\n\nThe main analysis is done by lm().\nWe then use other functions on the output of lm(). For example, we can use summary() to print commonly requested results:\n\nsummary(lm_out)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.807 -10.733   0.153   9.472  37.611 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -20.7634    12.0270  -1.726  0.08746 . \nx1            2.8688     1.6483   1.740  0.08495 . \nx2            2.5771     0.8555   3.012  0.00331 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.69 on 97 degrees of freedom\nMultiple R-squared:  0.1113,    Adjusted R-squared:  0.09295 \nF-statistic: 6.072 on 2 and 97 DF,  p-value: 0.003276\n\n\nConfidence intervals and variance-covariance matrix of the estimates can be obtained by confint() and vcov():\n\nconfint(lm_out)\n\n                  2.5 %   97.5 %\n(Intercept) -44.6335974 3.106785\nx1           -0.4025881 6.140222\nx2            0.8791251 4.275063\n\nvcov(lm_out)\n\n            (Intercept)            x1           x2\n(Intercept)  144.647827 -13.347713331 -7.460851584\nx1           -13.347713   2.716868988 -0.005532551\nx2            -7.460852  -0.005532551  0.731913160\n\n\nThere are many functions for other statistics, such as influential statistics and model comparison.\nIn SPSS, to do analysis, we usually use a dialog box from the pull down menu, select variables, check some checkboxes, use some buttons to open other dialog boxes and set other options, click OK, and all the requested results are in the output.\nI used to think that this approach is due to the graphical user interface (GUI), which is the strength of SPSS. I forgot that (a) the GUI is a “syntax generator,” and (b) the format of SPSS syntax we have nowadays is very similar to that in SPSS before it has a GUI. Actually, when I first learned SPSS in 90’s, I did not even have access to a PC version with text menu. Syntax command was the only way to do analysis in SPSS. For example, REGRESSION is the command, and all the checkboxes and options are values for subcommands, like arguments in R functions.\nSo, the common way we do analysis in SPSS, with one command for one analysis, is not due to the GUI. It has always been this way, at least in the version I used in early 90’s, before systems like Windows became popular.\nSo, for an analysis, such as multiple regression, one function, or many functions?\nWhen I write functions or develop packages, I generally adopt the do-one-thing-and-do-it-well principle, though what constitutes “one thing” is not always clear. This principle make it easy for me to write, debug, and maintain a function or package.\nHowever, for users who are used to using GUI, using one function to do many things in an analysis is conceptually similar to using a dialog box, thought without the dialog box. The many-function approach does not fit well with the experience in using a dialog box.\nIn R, we certainly can write a function that calls other functions, simulating commands like REGRESSION in SPSS.\nSo, I think this is not a debate of which approach is better. In R, we can do both, and let the users do analysis in whatever approach they like. For development, the do-one-thing-and-do-it-well approach is a better approach. However, for users, especially when developing GUI, the one-function approach may be more convenient to the users. The function in the one-function approach, like REGRESSION, is like a wrapper of a collection of functions: an interface to them.\nFor example, we can write an R function similar to REGRESSION in SPSS. In SPSS, if all the default options are what we need, this command is sufficient:\nREGRESSION\n /DEPENDENT y\n /ENTER x1 x2.\nTo request confidence intervals (confint() in R) and the variance-covariance matrix of the estimates (vcov() in R), this will do:\nREGRESSION\n /STATISTICS DEFAULT BCOV CI(95)\n /DEPENDENT y\n /ENTER x1 x2.\nA similar function can be written in R:\n\nregression(data = dat,\n           dep = \"y\",\n           ivs = c(\"x1\", \"x2\"))\n\nWe can write it in a more “R-way”:\n\nregression(data = dat,\n           model = y ~ x1 + x2)\n\nThe default printout is something similar to SPSS. It can be a list of tables (data frames) and a print method for printing the output.\nActually, we can still say that we are adopting the do-one-thing-and-do-it-well approach, although the “one thing” is “an interface to a set of functions.”\nI am not trying to argue that we should use this or that approach. They are not mutually exclusive. I am just wondering how to make using R by writing scripts more accessible to users who are used to GUI, while still keeping the do-one-thing-and-do-it-well principle. Writing these kinds of wrappers may also make it easier to create GUIs for them. For example, as long as ... is not used, a generic function can be developed to check the arguments of a function using its definition and then automatically generate a dialog box for it. For a wrapper with a lot of arguments, a configuration file can be used to customize the dialog box.\nP.S.: jamovi is already doing something similar. Behind the dialog boxes are kind of wrapper functions. However, though can be used in console, the modules are, naturally, supposed to be used inside jamovi."
  },
  {
    "objectID": "posts/plot_mod/index.html",
    "href": "posts/plot_mod/index.html",
    "title": "Plotting Moderation Effects With ggplot2",
    "section": "",
    "text": "There are some R packages that help researchers to plot moderation (interaction) effects: The linear relations between x (independent variable / predictor) and y (dependent variable / outcome variable) for two or more levels of w (moderator). For example, I have been using visreg for multiple regression models fitted by lm() for a long time. It is simple to use and supports both base R graphics and ggplot2. stdmod, which I maintained, also has the function plotmod for plotting simple effects in moderated regression. For structural equation modelling, semTools can be used to plot interaction for latent variables using plotProbe(). plotProbe() can also be used to on observed variables using this workaround.\nHowever, there may be case in which all we need is just two or more lines, and all we have are the simple effects: Two or more sets of intercepts of slopes.\nThis is how to plot the simple effect:\nSuppose we want to plot the simple effects of x on y conditional on w (the moderator). From the output of some functions, we have the slopes and intercepts when w is “Low” or “High”:\nw is “Low”: intercept = 2, slope = 1\nw is “High”: intercept = 3, slope = 2\nWrite a simple function to compute the points\n\n# Simple regression model\nxyline <- function(x, a, b) {a + b * x}\n\nSet the range for x:\n\n# Range of x\nx <- c(0, 10)\n\nCompute the predicted values of y at the lower and upper limit of the range of x, for each level of w, when all other predictors of y in the model, if any, are equal to zero:\n\n# Generate the two points when moderator = \"Low\"\ndat0 <- data.frame(Moderator = \"Low\",\n                   x = x,\n                   y = xyline(x, a = 2, b = 1))\n# Generate the two points when moderator = \"High\"\ndat1 <- data.frame(Moderator = \"High\",\n                   x = x,\n                   y = xyline(x, a = 3, b = 2))\n\nCombine the datasets:\n\ndat <- rbind(dat0, dat1)\ndat\n\n  Moderator  x  y\n1       Low  0  2\n2       Low 10 12\n3      High  0  3\n4      High 10 23\n\n\nDraw the lines using ggplot2:\n\nlibrary(ggplot2)\np <- ggplot(dat, aes(x = x, y = y, color = Moderator)) +\n            geom_line() +\n            scale_color_manual(values = c(\"Low\" = \"blue\", \"High\" = \"red\"))\np\n\n\n\n\n\n\n\n\nThis plot can then be modified as necessary:\n\np2 <- p + xlab(\"Independent Variable\") +\n          ylab(\"Dependent Variable\")\np2\n\n\n\n\n\n\n\n\nThis solution can be used for multiple regression or structural equation modelling.\nThough not as elegant as using packages devoted to plotting moderation effects, this solution may be good enough for some simple scenarios. I believe it can be further improved. However, if we want more, maybe it is better to use packages like semTools and visreg.\nP.S.: This post is based on a suggestion I posted to the Google Group for lavaan."
  },
  {
    "objectID": "posts/show_options_set_by_lavaan/index.html",
    "href": "posts/show_options_set_by_lavaan/index.html",
    "title": "Show Options Set by lavaan",
    "section": "",
    "text": "lavaan is a convenient tool for doing structural equation modelling in R (Rosseel, 2012). One of its strength is having “prepackaged” estimators, which are shortcuts to a set of options, such as “ML”, “MLR”, “MLMVS”, and others (Savalei & Rosseel, 2022). It also tries to set default values for options based on the model and data.\nHowever, probably due to my not-so-good memory, I sometimes forgot what the settings are for a model. Therefore, in the package semhelpinghands, I wrote the function show_more_options() to show some of the settings of the output of lavaan() and its wrappers, such as sem() and cfa().1\nThe function show_more_options() is very easy to use … because it accepts only one argument, the output of lavaan().\nThis is an example based on the example of lavaan::cfa():\nTo show the major options, just pass the output to show_more_options():\nThe column Call shows whether the default setting is used for each row, based of the call used when fitting the model. However, it is not always clear to me what the default values are.\nThe column Actual shows the values extracted by lavaan::lavInspect() or from the Options slot. These are what the default values stand for in the fitted model.\nMany of the entries are either (a) already available in the output of summary(), or (b) can be deduced from the output. However, I would like to have a table for quick reference, hence I wrote this function.\nSuppose \"MLR\" is used as the estimator:\nThe output shows the exact names of the options (e.g., \"robust.huber.white\" and \"yuan.bentler.mplus\"). They can complement the more readable output of summary() if we need to manually set these options, or want to know which values these options refer to when consulting the help page.\nFor example, summary() reports that \"Sandwich\" is the method used for standard errors, and show_more_options() shows that the exact name in the option is \"robust.huber.white\". This is useful because the word \"sandwich\" does not appear in the help page of lavOptions(), while the word \"robust.huber.white\" does. Some users may not know what \"Sandwich\" stands for.\nThis is a dataset for a path model, with missing data:\nSuppose we use only the default options to fit a path model:\nThe output shows that, by default, the mean structure is not modelled, listwise selection is used to handle missing data, and x variables (exogenous covariates, x in this example) are treated as fixed. This can be verified by the parameter estimates, in which the variance of x is a fixed parameter and hence has no standard error and no p-value:\nSuppose we set missing to \"FIML\":\nx variables are still treated as fixed, but now mean structure is modelled (required for FIML, full information maximum likelihood), even though I did not explicitly ask for it.\nOnly options I think are likely needed (by me) are included in the output.2 More may be added in the future. In any case, if other options are needed, they can be retrieved by lavaan::lavInspect() or from the Options slot of the output. In most cases I myself encountered, all I want is a simple function that is easy to remember and no need to set any arguments other than the lavaan output. If I need something else, I will just extract the information myself.\nThis function was inspired by a script I wrote to enumerate the options set by the prepackaged shortcuts. Interested readers can read this thread at the Google Group for lavaan and this gist, to check how options will be set for different combinations of estimator, data, and some other options."
  },
  {
    "objectID": "posts/the_first_post/index.html",
    "href": "posts/the_first_post/index.html",
    "title": "Reviving an Old Blog",
    "section": "",
    "text": "This is a “revival” of an old blog of mine on psychological research and methodology (hence the name Blogonresearch). Just anything on research and methodology that are useful to me, and maybe also useful to others.\nEdit 2023-01-28: This is actually the “second revival” as I switched from Hugo to Quarto in early 2023."
  }
]