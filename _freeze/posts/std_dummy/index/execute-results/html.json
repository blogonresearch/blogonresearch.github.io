{
  "hash": "eefe468eb4bbf0d92a256a29eb655523",
  "result": {
    "markdown": "---\ntitle: \"Standardize Variables Except For Dummy Variables, Using std_selected_boot()\"\nauthor: \"Shu Fai Cheung\"\ndate: '2023-06-25'\ncategories: [\"R\", \"regression\", \"categorical variables\", \"stdmod\", \"standardized\", \"bootstrapping\", \"confidence-intervals\"]\nbibliography: references.bib\ncsl: apa.csl\n---\n\n\nThis post shows one simple way\nto get meaningful standardized\nregression coefficients in\nmultiple linear regression\nwith dummy variables,\nwith\nappropriate confidence\nintervals, using `std_selected_boot()` from\nthe [`stdmod`](https://sfcheung.github.io/stdmod/) package.\n\n# \"Betas\"\n\nIt is common in my area, psychology, to\nreport standardized coefficients, the\nso-called \"betas,\" when reporting the\nresults of multiple\nregression or related methods. It is so\ncommon that some programs have the \"betas\"\nreported by default, alongside the\nunstandardized coefficients (the \"Bs\").\nHowever, even if it is justifiable to\ninterpret\neffects of some variables\nin the standardized metric,\ndummy variables, if any, should still\nnot be standardized;\n\n::: {.callout-warning}\n## Dummy Variables Standardized Are No Longer Dummy Variables\nThe dummy variable can\nno longer be interpreted as a dummy variable\nbecause it no longer takes either 0 or 1.\n:::\n\nSo, the \"betas\" for dummy variables,\nthough maybe not \"wrong\", are\n*not interpretable*.\n\nUnfortunately, standardizing\nthem is\nquite common (including in my\nown work).\n\nThe solutions are simple:\n\n::: {.callout-tip}\n## Do Not Standardize Dummy Variables\nStandardize\nother variables *except* for the dummy\nvariables.\n:::\n\nHowever, common statistical programs\nstandardize *all* variables. No\nchoice. All or none.\n\nTo do\nstandardization *right* when\ndummy variables are present,\nwe need to *manually*\ndo the standardization. Doable\nbut inconvenient.\n\nThe function `std_selected_boot()` from the\nR package `stdmod`, which I and my\ncollaborators developed\n[@cheung_improving_2022],\ncan do the standardization right\nwithout any major changes\nto the common workflow.\n\n# How to Use `std_selected_boot()`\n\n::: {.callout-tip}\n## Workflow\n1. Fit a regression model by `lm()` as\n  usual.\n\n2. Pass the output to `std_selected()`\nand select variables to be standardized.\n:::\n\nIt was designed to let\nusers have full control on which variables\nto standardize (see this\n[article](https://sfcheung.github.io/stdmod/articles/std_selected.html) on how).\nIt has an additional feature:\n\n::: {.callout-note}\n## What It Won't Do (Correctly)\nIt automatically skips factors and string variables\n  in standardization. That is,\n  their dummy variables will never be standardized.\n:::\n\n## Example\n\n### Do Regression As Usual\n\nSuppose this is the data set:[^nonnormal]\n\n[^nonnormal]: `x1` and `x2` are intentionally\ngenerated from nonnormal distributions, for\nillustration. Note that OLS\n(ordinary least squares) regression\ndoes *not* assume that the predictors\nare multivariate normal. Using nonnormal\npredictors violate no assumptions of\nOLS regression.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code for generating data\"}\nset.seed(1453243)\nn <- 200\ngroup <- sample(c(\"Gp1\", \"Gp2\", \"Gp3\"),\n                size = n,\n                prob = c(.30, .20, .50),\n                replace = TRUE)\nx1 <- (rchisq(n, 2) - 2) * 4 + 10\nx2 <- runif(n, 1, 10)\ny <- 10 + 0.45 / 1 * x1 + 4 / 1 * x2 + sapply(group, switch,\n                                   Gp1 = 1,\n                                   Gp2 = 8,\n                                   Gp3 = 6) + rnorm(n, 0, 10)\ndat <- data.frame(x1 = round(x1, 2),\n                  x2 = round(x2, 2),\n                  group,\n                  y = round(y, 2))\nwrite.csv(dat, \"dat.csv\")\n```\n:::\n\n\nThis is the [data file](dat.csv).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    x1   x2 group     y\n1 9.97 7.84   Gp1 44.99\n2 2.45 5.11   Gp1 47.27\n3 8.50 8.87   Gp2 60.00\n4 4.10 5.60   Gp1 37.05\n5 8.16 7.18   Gp3 45.51\n6 2.53 9.19   Gp1 54.71\n```\n:::\n:::\n\n\nThe variables `x1` and `x2` are continuous\npredictors. The variable `group` is\na string variable, with three possible\nvalues: `Gp1`, `Gp2`, and `Gp3`.\nThe outcome variable is `y`.\n\nThis is the regression model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_out <- lm(y ~ x1 + x2 + group, dat)\nsummary(lm_out)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x1 + x2 + group, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-42.755  -7.165   0.355   6.597  25.317 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   9.8205     2.2545   4.356 2.14e-05 ***\nx1            0.4080     0.1021   3.997 9.10e-05 ***\nx2            4.1642     0.3051  13.649  < 2e-16 ***\ngroupGp2      6.9809     2.0704   3.372 0.000901 ***\ngroupGp3      6.3161     1.7076   3.699 0.000282 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.49 on 195 degrees of freedom\nMultiple R-squared:  0.5385,\tAdjusted R-squared:  0.5291 \nF-statistic: 56.89 on 4 and 195 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n### Do Standardization Right\n\nInstall `stdmod` and load it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(stdmod)\n```\n:::\n\n\nIf we want to standardize all variables\nexcept for categorical variables, if any,\nwe just pass the output to `std_selected_boot()`,\nand set `to_standardize` to `~ .`. The\nright-hand side of `~` denotes the variables\nto be standardized. If set to `.`, then\nall numeric variables, including the\noutcome variable (`y`),\nwill be standardized.[^selectsome]\n\n[^selectsome]: Suppose we only want to\nstandardize `x1` and `y`, because `x2`\nis on a meaningful unit, we can\nset `to_standardize` to `~ x1 + y`.\nOrder does not matter.\n\nBut this is not just about the coefficient.\nThere is one issue with standardization:\n**confidence intervals**.\n\n::: {.callout-warning}\n## Beware of the *t*-based SE and CI\nIf a variable is standardized, the\nusual *t*-based standard errors and\nconfidence intervals of the\ncoefficients that involve it *may* be\nbiased.[^biased]\n:::\n\n[^biased]: Note that there are indeed\ncases in which they are still unbiased,\nand cases in which the biases are\nnegligible. See @yuan_biases_2011\nfor a detailed discussion.\n\nThis is because\n(a) they do not take into account the\nsampling variation of the standard deviations\nused in standardization [@yuan_biases_2011],\nand (b) the coefficients with standardization,\nthe \"betas\", are not normally distributed\n(though may be close to). Many statistical\nprograms do not report the confidence\nintervals for \"betas,\" for a good reason.\n\nThis is why `std_selected_boot()` enables\nnonparametric bootstrapping percentile\nconfidence\nintervals *by default*, just in case\nthe bias is large.\n\nTo have stable\nand reproducible confidence intervals,\ncall `set.seed()` before calling\n`std_selected_boot()` and set `nboot`\nto the desired number of bootstrap\nsamples (at least 2000 but 5000\nor more is recommended):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(870516)\nlm_out_std <- std_selected_boot(lm_out,\n                                to_standardize = ~ .,\n                                nboot = 5000)\n```\n:::\n\n\nWe can call `summary()` as usual to\nprint the results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm_out_std)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall to std_selected_boot():\nstd_selected_boot(lm_out = lm_out, to_standardize = ~., nboot = 5000)\n\nSelected variable(s) are centered by mean and/or scaled by SD\n- Variable(s) centered: y x1 x2 group\n- Variable(s) scaled: y x1 x2 group\n\n      centered_by scaled_by                            Note\ny        40.36635 15.286176 Standardized (mean = 0, SD = 1)\nx1        9.41020  7.342158 Standardized (mean = 0, SD = 1)\nx2        5.35520  2.454193 Standardized (mean = 0, SD = 1)\ngroup          NA        NA Nonnumeric                     \n\nNote:\n- Categorical variables will not be centered or scaled even if\n  requested.\n- Nonparametric bootstrapping 95% confidence intervals computed.\n- The number of bootstrap samples is 5000.\n\nCall:\nlm(formula = y ~ x1 + x2 + group, data = dat_mod)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.79695 -0.46875  0.02324  0.43155  1.65619 \n\nCoefficients:\n            Estimate CI Lower CI Upper Std. Error t value Pr(>|t|)    \n(Intercept) -0.28825 -0.42260 -0.16397    0.08545  -3.374 0.000895 ***\nx1           0.19598  0.10064  0.28696    0.04903   3.997  9.1e-05 ***\nx2           0.66856  0.58901  0.73888    0.04898  13.649  < 2e-16 ***\ngroupGp2     0.45668  0.22507  0.68592    0.13544   3.372 0.000901 ***\ngroupGp3     0.41319  0.20033  0.63209    0.11171   3.699 0.000282 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6863 on 195 degrees of freedom\nMultiple R-squared:  0.5385,\tAdjusted R-squared:  0.5291 \nF-statistic: 56.89 on 4 and 195 DF,  p-value: < 2.2e-16\n\nNote:\n- Estimates and their statistics are based on the data after\n  mean-centering, scaling, or standardization.\n- [CI Lower, CI Upper] are bootstrap percentile confidence intervals.\n- Std. Error are not bootstrap SEs.\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\nThe output has one additional section:\n\n- Variables that are standardized[^std], and variables that are not transformed.\n\n[^std]: `std_selected()` allows users to\ndo only mean-centering or scaling by\nstandard deviation. Meaning-centering\nand than scaling by standard deviation\nis equivalent to standardization.\n\nThe other sections are similar to those\nof a usual multiple regression.\nNote that the column `Estimate` is\nintentionally *not* labelled as `Beta`\nbecause it is possible that only some\nvariables are standardized. Labelling\nit as `Beta`, though common, is\nmisleading.\n\n### Interpret The Output\n\nIn the regression output, because `y`,\n`x1`, and `x2` are standardized, they\nare the usual \"betas.\" However, `group`\nis not standardized and so `groupGp2` and\n`groupGp3` still take only two possible\nvalues, 0 and 1, and so can still be\ninterpreted as usual.\n\nFor example, the coefficient of\n`groupGp2` is 0.457. That\nis, compared to `Gp1`, the reference\ngroup, the predicted value of `y` is\n0.457 SD (of `y`)\nhigher in `Gp2`.\n\nOn the other hand, the coefficient of\n`groupGp3` is 0.413. That\nis, compared to `Gp1`, the predicted\nvalue of `y` is 0.413\nSD (of `y`) higher in `Gp3`.\n\nIn short, the coefficients of all dummy\nvariables can be interpreted as usual,\nthough the difference in the outcome\nvariable (dependent variable) is in SD\nof this variable if it is standardized.\n\n# Why The Common Practice Is Problematic\n\n## Standardize All Variables\n\nAssume that we standardize *all*\nvariables, including the dummy\nvariables, as in some statistical\nprogram. To simulate this, I\nmanually create the dummy variables,\nstandardize them, and do regression:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\ndat$groupGp2 <- 0\ndat$groupGp3 <- 0\ndat$groupGp2 <- ifelse(dat$group == \"Gp2\", 1, 0)\ndat$groupGp3 <- ifelse(dat$group == \"Gp3\", 1, 0)\ndat_std <- dat[, c(\"x1\", \"x2\", \"groupGp2\", \"groupGp3\", \"y\")]\ndat_std <- as.data.frame(scale(dat_std))\nhead(dat_std)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           x1          x2   groupGp2   groupGp3          y\n1  0.07624462  1.01247114 -0.5220306 -0.9206479  0.3024726\n2 -0.94797747 -0.09991063 -0.5220306 -0.9206479  0.4516270\n3 -0.12396901  1.43216096  1.9060186 -0.9206479  1.2844056\n4 -0.72324789  0.09974764 -0.5220306 -0.9206479 -0.2169509\n5 -0.17027692  0.74354368 -0.5220306  1.0807606  0.3364903\n6 -0.93708149  1.56255004 -0.5220306 -0.9206479  0.9383413\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\npsych::describe(dat_std, range = FALSE, skew = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         vars   n mean sd   se\nx1          1 200    0  1 0.07\nx2          2 200    0  1 0.07\ngroupGp2    3 200    0  1 0.07\ngroupGp3    4 200    0  1 0.07\ny           5 200    0  1 0.07\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nlm_out_std_wrong <- lm(y ~ x1 + x2 + groupGp2 + groupGp3, dat_std)\n```\n:::\n\n\nThe following results are what found\nin common statistical programs that\nstandardize all variables, including\nthe dummy variables, to yield the \"betas\":\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nprintCoefmat(summary(lm_out_std_wrong)$coefficients,\n             zap.ind = 1:4,\n             P.values = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.000000   0.048525  0.0000  1.00000    \nx1          0.195980   0.049034  3.9968    9e-05 ***\nx2          0.668560   0.048983 13.6488  < 2e-16 ***\ngroupGp2    0.188080   0.055783  3.3717  0.00090 ***\ngroupGp3    0.206450   0.055817  3.6987  0.00028 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\nLet us compare the results.\n\n### *p*-values\n\nThe *p*-values are the same, which is expected:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\ncoef_std <- summary(lm_out_std)$coefficients\ncoef_std_wrong <- summary(lm_out_std_wrong)$coefficients\nround(cbind(`p-value (Std Selected)` = coef_std[, \"Pr(>|t|)\"],\n            `p-value (Std All)` = coef_std_wrong[, \"Pr(>|t|)\"]), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            p-value (Std Selected) p-value (Std All)\n(Intercept)                  0.001             1.000\nx1                           0.000             0.000\nx2                           0.000             0.000\ngroupGp2                     0.001             0.001\ngroupGp3                     0.000             0.000\n```\n:::\n:::\n\n\n### Coefficient Estimates\n\nHowever, the coefficients for the dummy\nvariables are\ndifferent:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nround(cbind(`Estimate (Std Selected)` = coef_std[, \"Estimate\"],\n            `Estimate (Std All)` = coef_std_wrong[, \"Estimate\"]), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate (Std Selected) Estimate (Std All)\n(Intercept)                  -0.288              0.000\nx1                            0.196              0.196\nx2                            0.669              0.669\ngroupGp2                      0.457              0.188\ngroupGp3                      0.413              0.206\n```\n:::\n:::\n\n\nThe coefficients of `groupGp2`\nand `groupGp3`, when they are standardized,\nare not meaningful.\nAfter standardization,\nthey are no longer either 0 or 1.\n\nFor example, the so-called \"beta\"\nof `groupGp2`, 0.188,\nis *not* the difference between\n`Gp2` and `Gp1` on standardized `y`.\nIt is the increase in standardized `y`\nwhen `groupGp2` \"increases by one SD\",\nwhich is meaningless.\n\nWhat's worse, the ranking of the\ncoefficients changed. If we interpret\nthe coefficients with dummy variables not\nstandardized, the `Gp2 vs. Gp1`\ndifference is larger than the\n`Gp3 vs. Gp1` difference\n(0.457 vs 0.413).\n\nHowever, if we interpret the coefficients\nwith dummy variables standardized,\nwe would have a different conclusion:\nthe `Gp2 vs. Gp1`\ndifference is *smaller* than the\n`Gp3 vs. Gp1` difference\n(0.188 vs 0.206),\nthough only slightly.\n\nThe reason is, there are more cases in\n`Gp3` than in `Gp2`. The differences\nin the SDs of the dummy variables are\nlarge enough to reverse the ranking:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(dat$group)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nGp1 Gp2 Gp3 \n 65  43  92 \n```\n:::\n\n```{.r .cell-code}\nsd(dat$groupGp2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4118533\n```\n:::\n\n```{.r .cell-code}\nsd(dat$groupGp3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4996481\n```\n:::\n:::\n\n\nI created the data to\ndemonstrate that this reversal in ranking\nis *possible*.\nThis may not happen in real data. However,\nhaving groups with different numbers of\ncases is probably the norm rather than\nthe exception in real data.[^stdok]\n\n[^stdok]: It can be argued that size\ndifferences\nshould be taken into account. They\nshould be, but probably not by\nstandardizing the dummy variables and\nmaking the coefficients not interpretable.\n\n## Use *t* Statistics Confidence Intervals\n\nSome programs gives confidence intervals\nof \"betas\" using *t* statistics. That is,\nstandardize variables, do regression,\nand form the confidence intervals,\nas if the standardized variables\nwere the original data.\n\nLet us compare the bootstrap confidence\nintervals with the usual OLS confidence\nintervals based on the *t*-statistics.\n\nFor the output of `std_selected_boot()`,\nwe can request the *t*-based confidence\nintervals by setting `type` to\n`\"lm\"` when calling `confint()`, the\nfunction commonly used to form\nconfidence intervals:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# OLS confidence intervals\nround(confint(lm_out_std, type = \"lm\"), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             2.5 % 97.5 %\n(Intercept) -0.457 -0.120\nx1           0.099  0.293\nx2           0.572  0.765\ngroupGp2     0.190  0.724\ngroupGp3     0.193  0.634\n```\n:::\n:::\n\n\nWithout setting `type`, the\nnonparametric bootstrap confidence\nintervals are returned (if available):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap Confidence Intervals\nround(confint(lm_out_std), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             2.5 % 97.5 %\n(Intercept) -0.423 -0.164\nx1           0.101  0.287\nx2           0.589  0.739\ngroupGp2     0.225  0.686\ngroupGp3     0.200  0.632\n```\n:::\n:::\n\n\nAs shown above, the confidence\nintervals of `x1` by the two methods\nare close to each other. However,\nthe bootstrap confidence interval\nof `x2` is narrower than the\n*t*-based confidence interval.\n\nWe can compare the widths of the\nconfidence intervals by examining\ntheir ratios (`ratio` = `CI_Width_t` / `CI_Width_boot`):\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nci_t <- confint(lm_out_std, type = \"lm\")\nci_b <- confint(lm_out_std)\nwidth_t <- ci_t[, 2] - ci_t[, 1]\nwidth_b <- ci_b[, 2] - ci_b[, 1]\nci_compare <- data.frame(CI_Width_t = width_t,\n                         CI_Width_boot = width_b)\nci_compare$ratio <- ci_compare[, 1] / ci_compare[, 2]\nround(ci_compare, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            CI_Width_t CI_Width_boot ratio\n(Intercept)      0.337         0.259 1.303\nx1               0.193         0.186 1.038\nx2               0.193         0.150 1.289\ngroupGp2         0.534         0.461 1.159\ngroupGp3         0.441         0.432 1.021\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\nThe widths of the\nconfidence intervals are nearly identical\nfor `x1`. However, for `x2`, the *t*-based\nconfidence interval is nearly 28.9% wider\nthan the bootstrap confidence interval.\n\nFor `groupGp2`, even though dummy variable\nis correctly not standardized, it can\nstill be affected because the outcome\nvariable, `y`, is standardized. Its\n*t*-based confidence interval is also wider\nthan the bootstrap confidence interval\nby about 15.9%.\n\n# Final Remarks\n\nWhat `std_selected_boot()` does can be\nimplemented in R code by researchers using\nbase R packages only.\nThe function was developed to make\nit easier for researchers to do\nstandardization right, without adopting\nthe all-or-none approach.\n\nMore information on `std_selected_boot()`\ncan be found from its\n[help page](https://sfcheung.github.io/stdmod/reference/std_selected.html).\n\n# Other Resources\n\nThere are other program or function\nthat also correctly skip categorical\nvariables when doing standardization.\nFor example, the linear regression\nprocedure in [jamovi](https://www.jamovi.org/)\nalso do not standardized the dummy\nvariables formed from categorical\nvariables when standardized estimates\nare requested.\n\nFor confidence intervals, other methods\nhave also been proposed to take into\naccount the sampling variability of\nthe standard deviations. Examples\nare the methods proposed by @dudgeon_improvements_2017\nand @jones_computing_2013, in addition\nto the one proposed by @yuan_biases_2011.\n\n# References\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}