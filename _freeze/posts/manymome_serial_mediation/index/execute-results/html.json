{
  "hash": "5cbdbd56c1ac6ae2a0c6efaa8983f6d2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Serial Mediation in R: A Tutorial\"\nauthor: \"Shu Fai Cheung\"\ndate: '2024-09-14'\ncategories: [\"R\", \"mediation\", \"manymome\", \"bootstrapping\"]\nbibliography: references.bib\nformat:\n  html:\n    toc: true\ncsl: apa.csl\n---\n\n::: {.cell}\n\n:::\n\n\n\nThis tutorial shows how to use the R\npackage [`manymome`](https://sfcheung.github.io/manymome)\n[@cheung_manymome_2024], a flexible\npackage for mediation analysis,\nto test indirect effects in\na serial mediation model fitted by\nmultiple regression.\n\n# Pre-Requisite\n\nReaders are expected to have basic R\nskills and know how to fit a linear\nregression model using `lm()`.\n\nThe package `manymome` can be installed\nfrom CRAN:\n\n```r\ninstall.packages(\"manymome\")\n```\n\n# Data\n\nThis is the data file for illustration,\nfrom `manymome`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(manymome)\nhead(data_serial, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          x       m1        m2         y         c1       c2\n1 12.123280 20.57654  9.327606  9.002655  0.1092621 6.011779\n2  9.811849 18.20839  9.467283 11.561813 -0.1240136 6.423912\n3 10.111538 20.34305 10.054767  9.348610  4.2786083 5.336944\n```\n\n\n:::\n:::\n\n\n\n# A Serial Mediation Model\n\nFirst we fit a serial mediation\nmodel using\nonly `x`, `m1`, `m2`, and `y`.\n\n\n\n::: {.cell layout-align=\"center\" fig-heigh='3'}\n::: {.cell-output-display}\n![A Serial Mediation Model](index_files/figure-html/fig-full-model-1.png){#fig-full-model fig-align='center' width=768}\n:::\n:::\n\n\n\nIn this model:\n\n- `x` is the predictor\n(independent variable).\n\n- `y` is the\noutcome variable (dependent variable).\n\n- `m1` and `m2` are the mediators.\n\nThe goal is to compute and test the\nfollowing indirect effects from\n`x` to `y`:\n\n- `x` -> `m1` -> `m2` -> `y`.\n\n- `x` -> `m1` -> `y`.\n\n- `x` -> `m2` -> `y`.\n\n# Fit the Models by `lm()`\n\nTo estimate all the regression coefficients,\njust fit three regression\nmodels:\n\n- Predict `m1` by `x` (@fig-m1-model).\n\n- Predict `m2` by `m1` and `x` (@fig-m2-model).\n\n- Predict `y` by `m1`, `m2`, and `x` (@fig-y-model).\n\n\n\n::: {.cell layout-align=\"center\" fig-heigh='3'}\n::: {.cell-output-display}\n![The Model Predicting `m1`](index_files/figure-html/fig-m1-model-1.png){#fig-m1-model fig-align='center' width=768}\n:::\n:::\n\n::: {.cell layout-align=\"center\" fig-heigh='3'}\n::: {.cell-output-display}\n![The Model Predicting `m2`](index_files/figure-html/fig-m2-model-1.png){#fig-m2-model fig-align='center' width=576}\n:::\n:::\n\n::: {.cell layout-align=\"center\" fig-heigh='3'}\n::: {.cell-output-display}\n![The Model Predicting `y`](index_files/figure-html/fig-y-model-1.png){#fig-y-model fig-align='center' width=576}\n:::\n:::\n\n\n\nThis can be easily done by `lm()` in R:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict m1\nmodel_m1 <- lm(m1 ~ x,\n               data = data_serial)\n# Predict m2\nmodel_m2 <- lm(m2 ~ m1 + x,\n               data = data_serial)\n# Predict y\nmodel_y <- lm(y ~ m2 + m1 + x,\n              data = data_serial)\n```\n:::\n\n\n\nThese are the regression results:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model_m1)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlm(formula = m1 ~ x, data = data_serial)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 10.15035    0.97278  10.434  < 2e-16 ***\nx            0.82684    0.09694   8.529 1.86e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9211 on 98 degrees of freedom\nMultiple R-squared:  0.4261,\tAdjusted R-squared:  0.4202 \nF-statistic: 72.75 on 1 and 98 DF,  p-value: 1.86e-13\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model_m2)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlm(formula = m2 ~ m1 + x, data = data_serial)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.2075     1.4396   1.533    0.128    \nm1            0.5295     0.1029   5.146 1.39e-06 ***\nx            -0.2122     0.1303  -1.628    0.107    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9382 on 97 degrees of freedom\nMultiple R-squared:  0.2463,\tAdjusted R-squared:  0.2307 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model_y)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlm(formula = y ~ m2 + m1 + x, data = data_serial)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   8.5543     3.0282   2.825  0.00575 **\nm2            0.5736     0.2110   2.718  0.00779 **\nm1           -0.4149     0.2413  -1.720  0.08870 . \nx             0.4654     0.2746   1.695  0.09331 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.95 on 96 degrees of freedom\nMultiple R-squared:  0.08713,\tAdjusted R-squared:  0.05861 \nF-statistic: 3.054 on 3 and 96 DF,  p-value: 0.03212\n```\n\n\n:::\n:::\n\n\n\nThe direct effect is the\ncoefficient of `x` in the\nmodel predicting `y`, which\nis 0.465, and\nnot significant.\n\n# The Indirect Effects\n\nThe three indirect effects are computed\nfrom the products:\n\n- `x` -> `m1` -> `m2` -> `y`\n\n    - The product of `a1`-path, `b1`-path,\n      and `b2`-path.\n\n- `x` -> `m1` -> `y`\n\n    - The product of `a1`-path and `b3`-path.\n\n- `x` -> `m2` -> `y`\n\n    - The product of `a2`-path and `b2`-path.\n\nTo test\nthese indirect effects, one common method\nis nonparametric bootstrapping [@cheung_comparison_2009\n@mackinnon_comparison_2002].\nThis can be done easily by `indirect_effect()`\nfrom the package `manymome`.\n\n## Combine the Regression Results\n\nWe first combine the regression models\nby [`lm2list()`](https://sfcheung.github.io/manymome/reference/lm2list.html)\ninto one object to represent the whole model\n(@fig-full-model):[^order]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_model <- lm2list(model_m1,\n                      model_m2,\n                      model_y)\nfull_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nThe models:\nm1 ~ x\nm2 ~ m1 + x\ny ~ m2 + m1 + x\n```\n\n\n:::\n:::\n\n\n\n[^order]: The order does not matter when using\n`lm2list`.\n\n## Compute and Test the Indirect Effect\n\nFor this serial mediation model, we can simply use\n[`indirect_effect()`](https://sfcheung.github.io/manymome/reference/cond_indirect.html)\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nind12 <- indirect_effect(x = \"x\",\n                         y = \"y\",\n                         m = c(\"m1\", \"m2\"),\n                         fit = full_model,\n                         boot_ci = TRUE,\n                         R = 5000,\n                         seed = 3456)\n```\n:::\n\n\n\nThese are the main arguments:\n\n- `x`: The name of the `x` variable,\n  the start of the indirect path.\n\n- `y`: The name of the `y` variable,\n  the end of the indirect path.\n\n- `m`: A character vector of the names\n  of the mediators. The path goes from\n  the first name to the last name.\n  In the example above, the path is\n  `x -> m1 -> m2 -> y`. Therefore,\n  `m` is set to `c(\"m1\", \"m2\")`.\n\n- `fit`: The regression models combined\n  by `lm2list()`.\n\n- `boot_ci`: If `TRUE`, bootstrap\n  confidence interval will be formed.\n\n- `R`, the number of bootstrap samples.\n  It is fast for regression models and\n  I recommend using at least 5000\n  bootstrap samples or even 10000, because\n  the results may not be stable enough\n  if indirect effect is close to zero\n  [an example can be found in @cheung_semlbci_2023].\n\n- `seed`: The seed for the random number\n  generator, to make the resampling\n  reproducible. This argument should\n  always be set when doing bootstrapping.\n\nBy default, parallel processing will\nbe used and a progress bar will be\ndisplayed.\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nind1 <- indirect_effect(x = \"x\",\n                        y = \"y\",\n                        m = \"m1\",\n                        fit = full_model,\n                        boot_ci = TRUE,\n                        R = 5000,\n                        seed = 3456)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nind2 <- indirect_effect(x = \"x\",\n                        y = \"y\",\n                        m = \"m2\",\n                        fit = full_model,\n                        boot_ci = TRUE,\n                        R = 5000,\n                        seed = 3456)\n```\n:::\n\n\n\nJust typing the name of the output can\nprint the major results\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nind12\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n== Indirect Effect  ==\n                                       \n Path:               x -> m1 -> m2 -> y\n Indirect Effect:    0.251             \n 95.0% Bootstrap CI: [0.075 to 0.500]  \n\nComputation Formula:\n  (b.m1~x)*(b.m2~m1)*(b.y~m2)\nComputation:\n  (0.82684)*(0.52946)*(0.57361)\n\nPercentile confidence interval formed by nonparametric bootstrapping\nwith 5000 bootstrap samples.\n\nCoefficients of Component Paths:\n  Path Coefficient\n  m1~x       0.827\n m2~m1       0.529\n  y~m2       0.574\n```\n\n\n:::\n\n```{.r .cell-code}\nind1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n== Indirect Effect  ==\n                                       \n Path:               x -> m1 -> y      \n Indirect Effect:    -0.343            \n 95.0% Bootstrap CI: [-0.672 to -0.015]\n\nComputation Formula:\n  (b.m1~x)*(b.y~m1)\nComputation:\n  (0.82684)*(-0.41495)\n\nPercentile confidence interval formed by nonparametric bootstrapping\nwith 5000 bootstrap samples.\n\nCoefficients of Component Paths:\n Path Coefficient\n m1~x       0.827\n y~m1      -0.415\n```\n\n\n:::\n\n```{.r .cell-code}\nind2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n== Indirect Effect  ==\n                                      \n Path:               x -> m2 -> y     \n Indirect Effect:    -0.122           \n 95.0% Bootstrap CI: [-0.376 to 0.055]\n\nComputation Formula:\n  (b.m2~x)*(b.y~m2)\nComputation:\n  (-0.21223)*(0.57361)\n\nPercentile confidence interval formed by nonparametric bootstrapping\nwith 5000 bootstrap samples.\n\nCoefficients of Component Paths:\n Path Coefficient\n m2~x      -0.212\n y~m2       0.574\n```\n\n\n:::\n:::\n\n\n\nAs shown above, the indirect effect\nthrough `m1` and then `m2` is\n0.251.\nThe 95% bootstrap confidence interval is\n[0.075; 0.500]. The indirect effect\nis positive and significant.\n\nThe indirect effect\nthrough `m1` only is\n-0.343.\nThe 95% bootstrap confidence interval is\n[-0.672; -0.015]. The indirect effect\nis negative and significant.\n\nThe indirect effect\nthrough `m2` only is\n-0.122.\nThe 95% bootstrap confidence interval is\n[-0.376; 0.055]. The indirect effect\nis not significant.\n\nFor transparency, the output also shows\nhow the indirect effect was computed.\n\n## Standardized Indirect Effect\n\nTo compute and test the standardized\nindirect effects, with both the `x`-variable\nand `y`-variable standardized, add\n`standardized_x = TRUE` and\n`standardized_y = TRUE`:\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nind12_stdxy <- indirect_effect(x = \"x\",\n                               y = \"y\",\n                               m = c(\"m1\", \"m2\"),\n                               fit = full_model,\n                               boot_ci = TRUE,\n                               R = 5000,\n                               seed = 3456,\n                               standardized_x = TRUE,\n                               standardized_y = TRUE)\nind1_stdxy <- indirect_effect(x = \"x\",\n                              y = \"y\",\n                              m = \"m1\",\n                              fit = full_model,\n                              boot_ci = TRUE,\n                              R = 5000,\n                              seed = 3456,\n                              standardized_x = TRUE,\n                              standardized_y = TRUE)\nind2_stdxy <- indirect_effect(x = \"x\",\n                              y = \"y\",\n                              m = \"m2\",\n                              fit = full_model,\n                              boot_ci = TRUE,\n                              R = 5000,\n                              seed = 3456,\n                              standardized_x = TRUE,\n                              standardized_y = TRUE)\n```\n:::\n\n\n\nThe results can be printed as usual:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nind12_stdxy\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n== Indirect Effect (Both 'x' and 'y' Standardized) ==\n                                       \n Path:               x -> m1 -> m2 -> y\n Indirect Effect:    0.119             \n 95.0% Bootstrap CI: [0.038 to 0.228]  \n\nComputation Formula:\n  (b.m1~x)*(b.m2~m1)*(b.y~m2)*sd_x/sd_y\nComputation:\n  (0.82684)*(0.52946)*(0.57361)*(0.95489)/(2.00967)\n\nPercentile confidence interval formed by nonparametric bootstrapping\nwith 5000 bootstrap samples.\n\nCoefficients of Component Paths:\n  Path Coefficient\n  m1~x       0.827\n m2~m1       0.529\n  y~m2       0.574\n\nNOTE:\n- The effects of the component paths are from the model, not\n  standardized.\n```\n\n\n:::\n\n```{.r .cell-code}\nind1_stdxy\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n== Indirect Effect (Both 'x' and 'y' Standardized) ==\n                                       \n Path:               x -> m1 -> y      \n Indirect Effect:    -0.163            \n 95.0% Bootstrap CI: [-0.317 to -0.008]\n\nComputation Formula:\n  (b.m1~x)*(b.y~m1)*sd_x/sd_y\nComputation:\n  (0.82684)*(-0.41495)*(0.95489)/(2.00967)\n\nPercentile confidence interval formed by nonparametric bootstrapping\nwith 5000 bootstrap samples.\n\nCoefficients of Component Paths:\n Path Coefficient\n m1~x       0.827\n y~m1      -0.415\n\nNOTE:\n- The effects of the component paths are from the model, not\n  standardized.\n```\n\n\n:::\n\n```{.r .cell-code}\nind2_stdxy\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n== Indirect Effect (Both 'x' and 'y' Standardized) ==\n                                      \n Path:               x -> m2 -> y     \n Indirect Effect:    -0.058           \n 95.0% Bootstrap CI: [-0.176 to 0.025]\n\nComputation Formula:\n  (b.m2~x)*(b.y~m2)*sd_x/sd_y\nComputation:\n  (-0.21223)*(0.57361)*(0.95489)/(2.00967)\n\nPercentile confidence interval formed by nonparametric bootstrapping\nwith 5000 bootstrap samples.\n\nCoefficients of Component Paths:\n Path Coefficient\n m2~x      -0.212\n y~m2       0.574\n\nNOTE:\n- The effects of the component paths are from the model, not\n  standardized.\n```\n\n\n:::\n:::\n\n\n\nThe standardized indirect effect\nthrough `m1` and then `m2`\nis\n0.119.\nThe 95% bootstrap confidence interval is\n[0.038; 0.228], significant.\n\nThe standardized indirect effect\nthrough `m1`\nis\n-0.163.\nThe 95% bootstrap confidence interval is\n[-0.317; -0.008], negative and significant.\n\nThe standardized indirect effect\nthrough `m2`\nis\n-0.058.\nThe 95% bootstrap confidence interval is\n[-0.176; 0.025], not significant.\n\n# Total Indirect Effect\n\nSuppose we would like to compute the\ntotal indirect effects from `x` to `y`\nthrough all the paths involving `m1`\nand `m2`. This can be done by \"adding\"\nthe indirect effects computed above,\nsimply by using the `+` operator:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nind_total <- ind12 + ind1 + ind2\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nind_total\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n== Indirect Effect  ==\n                                        \n Path:                x -> m1 -> m2 -> y\n Path:                x -> m1 -> y      \n Path:                x -> m2 -> y      \n Function of Effects: -0.214            \n 95.0% Bootstrap CI:  [-0.541 to 0.093] \n\nComputation of the Function of Effects:\n ((x->m1->m2->y)\n+(x->m1->y))\n+(x->m2->y) \n\n\nPercentile confidence interval formed by nonparametric bootstrapping\nwith 5000 bootstrap samples.\n```\n\n\n:::\n:::\n\n\n\nThe standardized total indirect effect\ncan be computed similarly:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nind_total_stdxy <- ind12_stdxy + ind1_stdxy + ind2_stdxy\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nind_total_stdxy\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n== Indirect Effect (Both 'x' and 'y' Standardized) ==\n                                        \n Path:                x -> m1 -> m2 -> y\n Path:                x -> m1 -> y      \n Path:                x -> m2 -> y      \n Function of Effects: -0.102            \n 95.0% Bootstrap CI:  [-0.262 to 0.043] \n\nComputation of the Function of Effects:\n ((x->m1->m2->y)\n+(x->m1->y))\n+(x->m2->y) \n\n\nPercentile confidence interval formed by nonparametric bootstrapping\nwith 5000 bootstrap samples.\n```\n\n\n:::\n:::\n\n\n\nThe total indirect effect is not significant\nin this example. Nevertheless, this should be\ninterpreted with cautions because the paths\nare not of the same sign, some positive and\nsome negative. This is an example of\ninconsistent mediation.\n\n# A Serial Mediation Model With Some Control Variables\n\nSuppose we want to fit a more complicated\nmodel, with some other variables included,\nsuch as control variables `c1` and `c2`\nin the dataset (@fig-full-model2).\n\n\n\n::: {.cell layout-align=\"center\" fig-caption='A Serial Mediation Model With Control Variables' fig-heigh='9'}\n::: {.cell-output-display}\n![](index_files/figure-html/fig-full-model2-1.png){#fig-full-model2 fig-align='center' width=768}\n:::\n:::\n\n\n\nAlthough there are more predictors (`c1`\nand `c2`) and more direct and indirect\npaths (e.g., `c1` to `y` through `m1`),\nthere are still only just three regression\nmodels. We can fit them as usual by\n`lm()`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2_m1 <- lm(m1 ~ x + c1 + c2,\n                data = data_serial)\nmodel2_m2 <- lm(m2 ~ m1 + x + c1 + c2,\n                data = data_serial)\nmodel2_y <- lm(y ~ m1 + m2 + x + c1 + c2,\n               data = data_serial)\n```\n:::\n\n\n\nThese are the regression results:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model2_m1)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlm(formula = m1 ~ x + c1 + c2, data = data_serial)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 10.81566    1.14379   9.456 2.20e-15 ***\nx            0.82244    0.09424   8.727 8.03e-14 ***\nc1           0.17148    0.09070   1.891   0.0617 .  \nc2          -0.18886    0.09278  -2.036   0.0446 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.888 on 96 degrees of freedom\nMultiple R-squared:  0.4774,\tAdjusted R-squared:  0.4611 \nF-statistic: 29.23 on 3 and 96 DF,  p-value: 1.629e-13\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model2_m2)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlm(formula = m2 ~ m1 + x + c1 + c2, data = data_serial)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.51937    1.58681   2.218  0.02895 *  \nm1           0.42078    0.10188   4.130  7.8e-05 ***\nx           -0.11610    0.12598  -0.922  0.35909    \nc1           0.27753    0.09221   3.010  0.00335 ** \nc2          -0.16195    0.09460  -1.712  0.09017 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8864 on 95 degrees of freedom\nMultiple R-squared:  0.341,\tAdjusted R-squared:  0.3133 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model2_y)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlm(formula = y ~ m1 + m2 + x + c1 + c2, data = data_serial)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  9.46788    3.60900   2.623   0.0102 *\nm1          -0.43534    0.24539  -1.774   0.0793 .\nm2           0.52077    0.22753   2.289   0.0243 *\nx            0.49285    0.28063   1.756   0.0823 .\nc1           0.09884    0.21403   0.462   0.6453  \nc2          -0.09604    0.21300  -0.451   0.6531  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.966 on 94 degrees of freedom\nF-statistic: 1.893 on 5 and 94 DF,  p-value: 0.1028\n```\n\n\n:::\n:::\n\n\n\nWe then just combine them by `lm2list()`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_model2 <- lm2list(model2_m1,\n                       model2_m2,\n                       model2_y)\nfull_model2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nThe models:\nm1 ~ x + c1 + c2\nm2 ~ m1 + x + c1 + c2\ny ~ m1 + m2 + x + c1 + c2\n```\n\n\n:::\n:::\n\n\n\nThe indirect effects can be computed\nand tested in exactly the same way:\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nind2_12 <- indirect_effect(x = \"x\",\n                           y = \"y\",\n                           m = c(\"m1\", \"m2\"),\n                           fit = full_model2,\n                           boot_ci = TRUE,\n                           R = 5000,\n                           seed = 3456)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nind2_1 <- indirect_effect(x = \"x\",\n                          y = \"y\",\n                          m = \"m1\",\n                          fit = full_model2,\n                          boot_ci = TRUE,\n                          R = 5000,\n                          seed = 3456)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nind2_2 <- indirect_effect(x = \"x\",\n                          y = \"y\",\n                          m = \"m2\",\n                          fit = full_model2,\n                          boot_ci = TRUE,\n                          R = 5000,\n                          seed = 3456)\n```\n:::\n\n\n\nThis is the result:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nind2_12\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n== Indirect Effect  ==\n                                       \n Path:               x -> m1 -> m2 -> y\n Indirect Effect:    0.180             \n 95.0% Bootstrap CI: [0.032 to 0.385]  \n\nComputation Formula:\n  (b.m1~x)*(b.m2~m1)*(b.y~m2)\nComputation:\n  (0.82244)*(0.42078)*(0.52077)\n\nPercentile confidence interval formed by nonparametric bootstrapping\nwith 5000 bootstrap samples.\n\nCoefficients of Component Paths:\n  Path Coefficient\n  m1~x       0.822\n m2~m1       0.421\n  y~m2       0.521\n```\n\n\n:::\n\n```{.r .cell-code}\nind2_1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n== Indirect Effect  ==\n                                       \n Path:               x -> m1 -> y      \n Indirect Effect:    -0.358            \n 95.0% Bootstrap CI: [-0.704 to -0.018]\n\nComputation Formula:\n  (b.m1~x)*(b.y~m1)\nComputation:\n  (0.82244)*(-0.43534)\n\nPercentile confidence interval formed by nonparametric bootstrapping\nwith 5000 bootstrap samples.\n\nCoefficients of Component Paths:\n Path Coefficient\n m1~x       0.822\n y~m1      -0.435\n```\n\n\n:::\n\n```{.r .cell-code}\nind2_2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n== Indirect Effect  ==\n                                      \n Path:               x -> m2 -> y     \n Indirect Effect:    -0.060           \n 95.0% Bootstrap CI: [-0.267 to 0.099]\n\nComputation Formula:\n  (b.m2~x)*(b.y~m2)\nComputation:\n  (-0.11610)*(0.52077)\n\nPercentile confidence interval formed by nonparametric bootstrapping\nwith 5000 bootstrap samples.\n\nCoefficients of Component Paths:\n Path Coefficient\n m2~x      -0.116\n y~m2       0.521\n```\n\n\n:::\n:::\n\n\n\nThe indirect effect through\n`m1` and then `m2`\nis\n0.180.\nThe 95% bootstrap confidence interval is\n[0.032; 0.385], still\nstill significant after controlling for\nthe effects of `c1` and `c2`.\n\nThe indirect effect through only\n`m1`\nis\n-0.358.\nThe 95% bootstrap confidence interval is\n[-0.704; -0.018]. The indirect effect\nthrough only `m2` is -0.060.\nThe 95% bootstrap confidence interval is\n[-0.267; 0.099]. Both indirect effects\nare not significant after controlling\nfor the effects of `c1` and `c2`.\n\nStandardized indirect effects can also\nbe computed and tested just by adding\n`standardized_x = TRUE` and\n`standardized_y = TRUE`.\n\nThe total indirect effect can also be\ncomputed using `+`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nind2_total <- ind2_12 + ind2_1 + ind2_2\nind2_total\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n== Indirect Effect  ==\n                                        \n Path:                x -> m1 -> m2 -> y\n Path:                x -> m1 -> y      \n Path:                x -> m2 -> y      \n Function of Effects: -0.238            \n 95.0% Bootstrap CI:  [-0.585 to 0.074] \n\nComputation of the Function of Effects:\n ((x->m1->m2->y)\n+(x->m1->y))\n+(x->m2->y) \n\n\nPercentile confidence interval formed by nonparametric bootstrapping\nwith 5000 bootstrap samples.\n```\n\n\n:::\n:::\n\n\n\nAgain, this should be interpreted with\ncautions because the paths are not of\nthe same sign.\n\n# No Limit On The Number of Mediators\n\nAlthough the example above only has two\nmediators,\nthere is no limit on the\nnumber of mediators in the serial\nmediation model. Just fit all the\nregression models predicting the mediators,\ncombine them by `lm2list()`, and compute\nthe indirect effect as illustrated above\nfor each path.\n\n# Easy To Fit Models With Only Some Paths Included\n\nAlthough `x` points to all `m` variables\nin the model above, and control variables predict\nall variables other than `x`, it is easy\nto fit a model with only paths theoretically\nmeaningful:\n\n- Just fit the desired models\nby `lm()` and use `indirect_effect()` as\nusual.\n\nFor example, suppose this is the desired\nmodel (@fig-full-model3):\n\n\n\n::: {.cell layout-align=\"center\" fig-caption='A Serial Mediation Model With Some Paths Omitted' fig-heigh='9'}\n::: {.cell-output-display}\n![](index_files/figure-html/fig-full-model3-1.png){#fig-full-model3 fig-align='center' width=768}\n:::\n:::\n\n\n\nThe control variable `c1` only predicts\n`m1` and `m2`, and the control variable\n`c2` only predicts `y`, and the only\nindirect path is `x1 -> m1 -> m2 -> y`.\n\nWe just fit the three models using `lm()`\nbased on the hypothesized model:\n\n```r\n# Predict m1\nmodel_m1 <- lm(m1 ~ x + c1,\n               data = data_serial)\n# Predict m2\nmodel_m2 <- lm(m2 ~ m1 + c1,\n               data = data_serial)\n# Predict y\nmodel_y <- lm(y ~ m2 + x + c2,\n              data = data_serial)\n# Combine the models\nfull_model <- lm2list(model_m1,\n                      model_m2,\n                      model_y)\n```\n\nThen the indirect effect can be computed\nas before.\n\n# Advanced Topics\n\n## Customize the Printout\n\nThe printout can be customized in\nseveral ways. For example:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(ind12,\n      digits = 2,\n      pvalue = TRUE,\n      pvalue_digits = 3,\n      se = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n== Indirect Effect  ==\n                                       \n Path:               x -> m1 -> m2 -> y\n Indirect Effect:    0.25              \n 95.0% Bootstrap CI: [0.08 to 0.50]    \n Bootstrap p-value:  0.004             \n Bootstrap SE:       0.11              \n\nComputation Formula:\n  (b.m1~x)*(b.m2~m1)*(b.y~m2)\nComputation:\n  (0.82684)*(0.52946)*(0.57361)\n\nPercentile confidence interval formed by nonparametric bootstrapping\nwith 5000 bootstrap samples.\nStandard error (SE) based on nonparametric bootstrapping with 5000\nbootstrap samples.\n\nCoefficients of Component Paths:\n  Path Coefficient\n  m1~x        0.83\n m2~m1        0.53\n  y~m2        0.57\n```\n\n\n:::\n:::\n\n\n\n- `digits`: The number of digits after\n  the decimal point for major output.\n  Default is 3.\n\n- `pvalue`: Whether bootstrapping\n  *p*-value is printed. The method\n  by @asparouhov_bootstrap_2021 is used.\n\n- `pvalue_digits`: The number of digits\n  after the decimal point for the\n  *p*-value. Default is 3.\n\n- `se`: The standard error based on\n  bootstrapping (i.e., the standard\n  deviation of the bootstrap estimates).\n\n## Missing Data\n\nCare needs to be taken if missing data\nis present. Let's remove some data\npoints from the data file:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_serial_missing <- data_serial\ndata_serial_missing[1:3, \"x\"] <- NA\ndata_serial_missing[2:4, \"m1\"] <- NA\ndata_serial_missing[3:5, \"m2\"] <- NA\ndata_serial_missing[3:6, \"y\"] <- NA\nhead(data_serial_missing)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          x       m1       m2         y           c1       c2\n1        NA 20.57654 9.327606  9.002655  0.109262124 6.011779\n2        NA       NA 9.467283 11.561813 -0.124013582 6.423912\n3        NA       NA       NA        NA  4.278608266 5.336944\n4 10.071555       NA       NA        NA  1.245356016 5.589547\n5 11.912888 20.54746       NA        NA -0.000932131 5.339643\n6  9.126304 16.53879 8.933963        NA  1.802726620 5.905691\n```\n\n\n:::\n:::\n\n\n\nIf we do the regression separately,\nthe cases used in the two models will\nbe different:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict m1\nmodel_m1_missing <- lm(m1 ~ x,\n                      data = data_serial_missing)\nmodel_m2_missing <- lm(m2 ~ m1 + x,\n                      data = data_serial_missing)\n# Predict y\nmodel_y_missing <- lm(y ~ m2 + m1 + x,\n                      data = data_serial_missing)\n```\n:::\n\n\n\nThe sample sizes are not the same:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnobs(model_m1_missing)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 96\n```\n\n\n:::\n\n```{.r .cell-code}\nnobs(model_m2_missing)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 95\n```\n\n\n:::\n\n```{.r .cell-code}\nnobs(model_y_missing)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 94\n```\n\n\n:::\n:::\n\n\n\nIf they are combined by `lm2list()`,\nan error will occur. The function `lm2list()`\nwill compare the data to see if the cases\nused are likely to be different.[^compare]\n\n[^compare]: The function `lm2list()` checks\nnot only sample sizes. Even if the sample\nsizes are the same, an error will still\nbe raised if there is evidence suggesting\nthat the samples are not the same, such\nas different values of `x` in the two\nmodels.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm2list(model_m1_missing,\n        model_m2_missing,\n        model_y_missing)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in check_lm_consistency(...): The data sets used in the lm models do not have identical sample size. All lm models must be fitted to the same sample.\n```\n\n\n:::\n:::\n\n\n\nA simple (though not ideal) solution is\nto use listwise deletion, keeping only\ncases with complete data. This can be done\nby `na.omit()`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_serial_listwise <- na.omit(data_serial_missing)\nhead(data_serial_listwise)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           x       m1        m2         y       c1       c2\n7  10.155132 17.18210  7.492526 10.621663 2.308957 5.782383\n8   9.086526 18.43523  8.259809 11.947661 1.143850 5.697380\n9  10.826916 19.79991 12.235185 11.118033 2.394711 4.036630\n10 10.422038 17.62173 10.742770  9.863857 2.390169 5.247402\n11  9.320105 18.49081 11.881927  9.730757 2.447613 4.294859\n12  7.815929 16.07152  8.566832 12.520207 1.331264 4.676967\n```\n\n\n:::\n\n```{.r .cell-code}\nnrow(data_serial_listwise)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 94\n```\n\n\n:::\n:::\n\n\n\nThe number of cases using listwise deletion\nis 94, less than\nthe full sample with missing data\n(94).\n\nThe steps above can then be proceed as\nusual.\n\n# Functions Used In This Example\n\nThese are the main functions used:\n\n- [`lm2list()`](https://sfcheung.github.io/manymome/reference/lm2list.html): Combining the results of\n  several one-outcome regression models.\n\n- [`indirect_effect()`](https://sfcheung.github.io/manymome/reference/cond_indirect.html): Compute\n  and test an indirect effect.\n\n# Further Information\n\nThe package `manymome` has no inherent\nlimitations on the number of variables and\nthe form of the mediation models. An\nillustration using a more complicated\nmodels with both parallel and serial\nmediation paths can be found in\n[this online article](https://sfcheung.github.io/manymome/articles/med_lm.html).\n\nOther features of `manymome` can be\nfound in [the website](https://sfcheung.github.io/manymome/)\nfor it.\n\n# Disclaimer: Similarity Across Tutorials\n\nTo keep each tutorial self-contained,\nsome sections are intentionally repeated\nnearly verbatim (\"recycled\")\nto reduce the hassle to read several articles\nto learn how to do one task.\n\n# Revision History and Issues\n\nThe revision history of this post can\nbe find in the [GitHub history of\nthe source file](https://github.com/blogonresearch/blogonresearch.github.io/commits/main/posts/manymome_serial_mediation/index.qmd).\n\nFor issues on this post, such as corrections\nand mistakes, please [open an issue](https://github.com/blogonresearch/blogonresearch.github.io/issues)\nfor the GitHub repository for this blog.\nThanks.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}