{
  "hash": "7712978edd349003d08407e22fb906cf",
  "result": {
    "markdown": "---\ntitle: \"Proper Standardization in Moderated Regression Using std_selected_boot()\"\nauthor: \"Shu Fai Cheung\"\ndate: '2023-07-01'\ncategories: [\"R\", \"regression\", \"moderation\", \"stdmod\", \"standardized\", \"bootstrapping\", \"confidence-intervals\"]\nbibliography: references.bib\ncsl: apa.csl\n---\n\n\nThis post shows one simple way\nto get correctly standardized\nregression coefficients in\nmultiple linear regression\nwith a moderator,\nwith\nappropriate confidence\nintervals, using `std_selected_boot()` from\nthe [`stdmod`](https://sfcheung.github.io/stdmod/) package.\n\n(*Note*: This post and some related posts,\ne.g., [this one on categorical variables](https://blogonresearch.github.io/posts/std_dummy/), have sections that are\nidentical or highly similar.\nThis is *intentional*, to make each\narticle self-contained. Readers do\nnot need to refer to other articles\nto understand the points.)\n\n# \"Betas\"\n\nIt is common in my area, psychology, to\nreport standardized coefficients, the\nso-called \"betas,\" when reporting the\nresults of multiple\nregression or related methods. It is so\ncommon that some programs have the \"betas\"\nreported by default, alongside the\nunstandardized coefficients (the \"Bs\").\n\nHowever, when a regression model has\na moderator and hence a product term,\nthe common practice, standardizing\nall variables, including the product\nterm, will lead to \"betas\" that is\ndifficult to interpret.\n\n::: {.callout-warning}\n## A Standardized Product Is Not A Product of Standardized Variables\nThat is, the standardized $x \\times w$ is not\n$\\textrm{standardized } x \\times \\textrm{standardized } w$.\nThe value of the former is no longer a measure of the\nmoderation effect.[^except]\n:::\n\n[^except]: Standardized $x \\times w$ can\nbe close to\n$\\textrm{standardized } x \\times \\textrm{standardized } w$,\nbut only in some very specific situations,\nas shown in @cheung_improving_2022.\n\nThis point is not new. @friedrich_defense_1982\nhas discussed this a long time ago, and\n@aiken_multiple_1991 have also explored\nthis in details.\n\nUnfortunately, reporting the \"beta\"\nof the product term computed this\nway is\nquite common (including in my\nown work).\n\nThe solutions are simple:\n\n::: {.callout-tip}\n## Standardize The Components\nFor the product term $x \\times w$,\nstandardize $x$ ($z_x$) and\n$w$ ($z_w$), and compute the\nproduct of them, $z_x \\times z_w$.\n:::\n\nHowever, common statistical programs\nstandardize *all* variables. To do\nstandardization *right* in moderated\nregression, we need to *manually*\ndo the standardization and compute\nthe product term, *twice*.\n\nThe function `std_selected_boot()` from the\nR package `stdmod`, which I and my\ncollaborators developed for moderated regression [@cheung_improving_2022],\ncan do the standardization as\npresented in @aiken_multiple_1991\nand @friedrich_defense_1982,\nwithout any drastic changes\nto the common workflow.\n\n# How to Use `std_selected_boot()`\n\n::: {.callout-tip}\n## Workflow\n1. Fit a regression model by `lm()` as\n  usual.\n\n2. Pass the output to `std_selected_boot()`\nand select variables to be standardized.\n:::\n\nIt was designed to let\nusers have full control on which variables\nto standardize (see this\n[article](https://sfcheung.github.io/stdmod/articles/std_selected.html) on how).\nIt has one additional feature:\n\n::: {.callout-note}\n## What It Won't Do (Correctly)\nIf a model has product terms, they will be\nformed *after* the component terms\nare standardized.\n:::\n\n## Example\n\n### Do Regression As Usual\n\nSuppose this is the data set:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code for generating data\"}\nset.seed(342532)\nn <- 200\ncontrol1 <- rnorm(n, 5, 1)\ncontrol2 <- rnorm(n, 20, 5)\nx <- rnorm(n, 10, 4)\nw <- rnorm(n, 10, 2)\ny <- 10 + 1 * control1 + 2 * control2 +\n     (3 + 1 * (w - 10)) * x + rnorm(n, 0, 30)\ndat <- data.frame(control1, control2, x, w, y)\nwrite.csv(dat, \"dat.csv\")\n```\n:::\n\n\nThis is the [data file](dat.csv).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  control1 control2         x         w         y\n1 5.025877 22.32080  9.521779 11.049711 131.15905\n2 6.200119 17.04644 11.912240 10.860813 102.01347\n3 4.992346 27.28403  9.845949 10.515219  77.86161\n4 4.979231 12.85510  6.512449 13.635488  89.54024\n5 7.020257 15.48984  5.734798  9.846383  66.94897\n6 3.969682 24.61428  3.329810 10.221247  38.20317\n```\n:::\n:::\n\n\nThe variable `x` is the predictor,\n`w` the moderator. The dataset also has\ntwo control variables, `control1` and\n`control2`. The outcome variable is\n`y`.\n\nThis is the regression model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_out <- lm(y ~ control1 + control2 + x*w, dat)\nsummary(lm_out)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ control1 + control2 + x * w, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-86.895 -18.374  -0.556  19.255  70.971 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  40.8781    29.5919   1.381 0.168746    \ncontrol1     -2.2750     2.1314  -1.067 0.287129    \ncontrol2      1.0053     0.3821   2.631 0.009191 ** \nx            -4.6922     2.3479  -1.998 0.047060 *  \nw             0.3635     2.4452   0.149 0.881986    \nx:w           0.7780     0.2287   3.402 0.000811 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28.34 on 194 degrees of freedom\nMultiple R-squared:  0.3587,\tAdjusted R-squared:  0.3422 \nF-statistic:  21.7 on 5 and 194 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\nThe moderation effect is significant.\n\n  - If `w` increases by one unit, the\n    effect of `x` increases by\n    0.778.\n\n  - If `w` is equal to zero, the effect\n    of `x` is -4.692, and\n    significant.\n\n### Do Standardization Right\n\nInstall `stdmod` and load it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(stdmod)\n```\n:::\n\n\nIf we want to standardize all variables\nexcept for the product term, *and*\ncompute the product term as the product\nof the *standardized* variables,\nwe just pass the output to\n`std_selected_boot()`,\nand set `to_standardize` to `~ .`. The\nright-hand side of `~` denotes the variables\nto be standardized. If set to `.`, then\nall numeric variables, including the\noutcome variable (`y`),\nwill be standardized.[^selectsome]\n\n[^selectsome]: Suppose we only want to\nstandardize `x` and `y`, because `w`\nis on a meaningful unit, we can\nset `to_standardize` to `~ x + y`.\nOrder does not matter.\n\nBut this is not just about the coefficient.\nThere is one issue with standardization:\n**confidence intervals**.\n\n::: {.callout-warning}\n## Beware of the *t*-based SE and CI\nIf a variable is standardized, the\nusual *t*-based standard errors and\nconfidence intervals of the\ncoefficients that involve it *may* be\nbiased.[^biased]\n:::\n\n[^biased]: Note that there are indeed\ncases in which they are still unbiased,\nand cases in which the biases are\nnegligible. See @yuan_biases_2011\nfor a detailed discussion.\n\nThis is because\n(a) they do not take into account the\nsampling variation of the standard deviations\nused in standardization [@yuan_biases_2011],\nand (b) the coefficients with standardization,\nthe \"betas\", are not normally distributed\n(though may be close to). Many statistical\nprograms do not report the confidence\nintervals for \"betas,\" for a good reason.\n\nThe case of moderated\nregression is more complicated\nbecause, as shown in\n@cheung_improving_2022, the correctly\nstandardized product term involves the\nstandard deviations of *three*\nvariables, not two.\n\nThis is why `std_selected_boot()` enables\nnonparametric bootstrapping percentile\nconfidence\nintervals *by default*, just in case\nthe bias is large.\n\nTo have stable\nand reproducible confidence intervals,\ncall `set.seed()` before calling\n`std_selected_boot()` and set `nboot`\nto the desired number of bootstrap\nsamples (at least 2000 but 5000\nor more is recommended):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(645321)\nlm_out_std <- std_selected_boot(lm_out,\n                                to_standardize = ~ .,\n                                nboot = 5000)\n```\n:::\n\n\nCall `summary()` as usual:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm_out_std)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall to std_selected_boot():\nstd_selected_boot(lm_out = lm_out, to_standardize = ~., nboot = 5000)\n\nSelected variable(s) are centered by mean and/or scaled by SD\n- Variable(s) centered: y control1 control2 x w\n- Variable(s) scaled: y control1 control2 x w\n\n         centered_by  scaled_by                            Note\ny          83.477789 34.9370599 Standardized (mean = 0, SD = 1)\ncontrol1    5.060548  0.9562693 Standardized (mean = 0, SD = 1)\ncontrol2   19.625484  5.3780625 Standardized (mean = 0, SD = 1)\nx           9.844108  4.3166239 Standardized (mean = 0, SD = 1)\nw          10.132063  2.0474763 Standardized (mean = 0, SD = 1)\n\nNote:\n- Categorical variables will not be centered or scaled even if\n  requested.\n- Nonparametric bootstrapping 95% confidence intervals computed.\n- The number of bootstrap samples is 5000.\n\nCall:\nlm(formula = y ~ control1 + control2 + x * w, data = dat_mod)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.48720 -0.52591 -0.01591  0.55113  2.03139 \n\nCoefficients:\n             Estimate  CI Lower  CI Upper Std. Error t value Pr(>|t|)    \n(Intercept)  0.020131 -0.005886  0.052164   0.057654   0.349 0.727347    \ncontrol1    -0.062269 -0.152822  0.036064   0.058339  -1.067 0.287129    \ncontrol2     0.154750  0.045436  0.265029   0.058813   2.631 0.009191 ** \nx            0.394150  0.277605  0.502798   0.059581   6.615 3.52e-10 ***\nw            0.470113  0.358552  0.573546   0.057982   8.108 5.71e-14 ***\nx:w          0.196803  0.098112  0.289331   0.057843   3.402 0.000811 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.811 on 194 degrees of freedom\nMultiple R-squared:  0.3587,\tAdjusted R-squared:  0.3422 \nF-statistic:  21.7 on 5 and 194 DF,  p-value: < 2.2e-16\n\nNote:\n- Estimates and their statistics are based on the data after\n  mean-centering, scaling, or standardization.\n- [CI Lower, CI Upper] are bootstrap percentile confidence intervals.\n- Std. Error are not bootstrap SEs.\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\nThe output has one additional section:\n\n- Variables that are standardized[^std], and variables that are not transformed.\n\n[^std]: `std_selected_boot()` allows users to\ndo only mean-centering or scaling by\nstandard deviation. Meaning-centering\nand than scaling by standard deviation\nis equivalent to standardization.\n\nAs shown in the first section, `x`\nand `w` are standardized. `x:w` is\nnot on the list. It is because `x:w`\nis not standardized. It should not be,\nas explained above.\n\nThe other sections are similar to those\nfor a usual multiple regression.\nNote that the column `Estimate` is\nintentionally *not* labelled as `Beta`\nbecause it is possible that only some\nvariables are standardized. Labelling\nit as `Beta`, though common, is\nmisleading.\n\n### Interpret The Output\n\nThe coefficient of the product term\n`x:w` is 0.197.\nThat is, for each one *SD* increase of `w`, the\n*standardized* effect of `x` on `y` (\nthe \"beta\" of `x` on `y`) increases by\n0.197.\n\nMy collaborator and I proposed\nto call the moderation effect with `x`,\n`w`, and `y` standardized,\n`x:w` in the example, the\n*standardized moderation effect*\n[@cheung_improving_2022].\nWhen variables are standardized as\nproposed by @friedrich_defense_1982,\nthe coefficients can be interpreted as\nusual in moderated regression, with all\nvariables on the standardized metric.\n\nThe coefficient of `x` is 0.394.\nThis is the *standardized* effect (the\n\"beta\") of `x` when `w` in this model is\nequal to zero. Because `w` is standardized,\nthis is equivalent to say that this is\nthe standardized effect of `x` when `w`\nis equal to its mean because the mean\nof a standardized variable is zero.\n\n### Conditional Effect\n\nThe function `cond_effect_boot()` from\n`stdmod` can be used to compute the\nconditional effects. Just pass\nthe output of `lm()` or\n`std_selected_boot()` to it.\n\nWhen the variables\nare standardized, `cond_effect_boot()`\ncan be used to\ncompute the *standardized*\nconditional effects, with\nnonparametric bootstrap confidence\nintervals. Just set\n`nboot` to the desired number of\nbootstrap samples. To ensure that\nthe same `nboot` bootstrap samples are\ndrawn, set the seed to the number used\nin `std_seleted_boot()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(645321)\ncond_std <- cond_effect_boot(lm_out_std,\n                             x = \"x\",\n                             w = \"w\",\n                             nboot = 5000)\ncond_std\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe effects of x on y, conditional on w:\n\n  Level      w x Effect CI Lower CI Upper  S.E.     t     p Sig\n   High  1.000    0.591    0.429    0.737 0.085 6.982 0.000 ***\n Medium  0.000    0.394    0.278    0.503 0.060 6.615 0.000 ***\n    Low -1.000    0.197    0.051    0.340 0.081 2.424 0.016 *  \n\n[CI Lower, CI Upper] shows the 95% nonparametric bootstrap confidence\ninterval(s) (based on 5000 bootstrap samples).\n\n\nThe regression model:\n\n\ty ~ control1 + control2 + x * w\n\nInterpreting the levels of w:\n\n  Level      w % Below From Mean (in SD)\n   High  1.000   81.50              1.00\n Medium  0.000   51.00              0.00\n    Low -1.000   15.00             -1.00\n\n- % Below: The percent of cases equal to or less than a level.\n- From Mean (in SD): Distance of a level from the mean, in standard\n  deviation (+ve above, -ve below).\n\nNote:\n\n- The variable(s) y, x, w is/are standardized.\n- The conditional effects are the standardized effects of x on y.\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\nAs shown above, the effect of `x`\nis positive and significant when `w`\nranges from one SD below mean to one SD\nabove mean. The *standardized* effect\nof `x` on `y` when `w` is one SD above\nmean is 0.591.\n\n# Why The Usual Practice Is Problematic\n\n## Standardize the Product Term\n\nAssume that we standardize *all*\nvariables, including the product term,\nas in\nsome statistical\nprogram. To simulate this, let's\nmanually create the product term,\nstandardize all variables, including\nthe product term, and do regression:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\ndat$x_w <- dat$x * dat$w\ndat_std <- as.data.frame(scale(dat))\nhead(dat_std)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     control1   control2             x           w          y        x_w\n1 -0.03625650  0.5011678 -0.0746715806  0.44818493  1.3647761  0.1360233\n2  1.19168346 -0.4795487  0.4791086834  0.35592570  0.5305451  0.6515372\n3 -0.07132155  1.4240348  0.0004264965  0.18713570 -0.1607514  0.1001691\n4 -0.08503578 -1.2588894 -0.7718205413  1.71109398  0.1735248 -0.2141245\n5  2.04932709 -0.7689844 -0.9519731607 -0.13952791 -0.4731027 -0.9039326\n6 -1.14075236  0.9276200 -1.5091188133  0.04355764 -1.2958909 -1.3825065\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\npsych::describe(dat_std, range = FALSE, skew = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         vars   n mean sd   se\ncontrol1    1 200    0  1 0.07\ncontrol2    2 200    0  1 0.07\nx           3 200    0  1 0.07\nw           4 200    0  1 0.07\ny           5 200    0  1 0.07\nx_w         6 200    0  1 0.07\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nlm_out_std_wrong <- lm(y ~ control1 + control2 + x + w + x_w, dat_std)\n```\n:::\n\n\nThe following results are what found\nin common statistical programs that\nstandardize all variables, including\nthe product term, to yield the \"betas\":\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nprintCoefmat(summary(lm_out_std_wrong)$coefficients,\n             zap.ind = 1:4,\n             P.values = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.000000   0.057350  0.0000  1.00000    \ncontrol1    -0.062270   0.058339 -1.0674  0.28713    \ncontrol2     0.154750   0.058813  2.6312  0.00919 ** \nx           -0.579740   0.290091 -1.9985  0.04706 *  \nw            0.021300   0.143300  0.1486  0.88199    \nx_w          1.043740   0.306771  3.4023  0.00081 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\nLet us compare the results.\n\n### *p*-values\n\nSome *p*-values are different, which is expected:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\ncoef_std <- summary(lm_out_std)$coefficients\ncoef_std_wrong <- summary(lm_out_std_wrong)$coefficients\nround(cbind(`p-value (Std Correct)` = coef_std[, \"Pr(>|t|)\"],\n            `p-value (Std All)` = coef_std_wrong[, \"Pr(>|t|)\"]), 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            p-value (Std Correct) p-value (Std All)\n(Intercept)               0.72735           1.00000\ncontrol1                  0.28713           0.28713\ncontrol2                  0.00919           0.00919\nx                         0.00000           0.04706\nw                         0.00000           0.88199\nx:w                       0.00081           0.00081\n```\n:::\n:::\n\n\nThe *p*-values of `x:w` are the same,\nwhich is expected [see @aiken_multiple_1991].\n\nHowever, the *p*-values of `x` and `w` are\nvery different. This is not just because\nthey are effects of `x` and `w` conditional\non other values. They actually are\n*not* conditional effects as we\nusually understood, explained\nin the next section.\n\n### Coefficient Estimates\n\nLet us compare the coefficients:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nround(cbind(`Estimate (Std Correct)` = coef_std[, \"Estimate\"],\n            `Estimate (Std All)` = coef_std_wrong[, \"Estimate\"]), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate (Std Correct) Estimate (Std All)\n(Intercept)                  0.020              0.000\ncontrol1                    -0.062             -0.062\ncontrol2                     0.155              0.155\nx                            0.394             -0.580\nw                            0.470              0.021\nx:w                          0.197              1.044\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\nThe coefficients of `x`, `w`, and `x:w`\nhave very different values between the\ncorrect standardized solution and the\nstandardized solution with product term\nstandardized.\n\nTwo points of note for the second\ncolumn, the solution with the product\nterm itself standardized:\n\n  1. The coefficient of `x:w`\n      , when it\n      is standardized\n      (called `x_w`), is not a\n      product term. The value\n      1.044 is\n      *not* the increase of effect of\n      `x` when `w` increases by one SD,\n      nor by one unit.\n\n  2. The coefficient of `x` is *not*\n      the usual conditional effect.\n      Mathematically,\n      it is the standardized effect of\n      `x` when `w` is equal to \"some\n      value\". However, what is this\n      value? This is not easy to\n      determine because `x_w` is not\n      a product term. The coefficient\n      of `w` has the same problem.\n\n::: {.callout-warning}\n## Standardizing the Product Term Make Interpretation Difficult\n  - The coefficient of the product term\n    is no longer the moderation effect\n    (except in some very special cases).\n  - The coefficients of the component\n    terms, the focal variable and the\n    moderator, are no longer the\n    conditional effects of values easily\n    deducible from the results.\n:::\n\n## Use *t* Statistics Confidence Intervals\n\nSome programs gives confidence intervals\nof \"betas\" using *t* statistics. That is,\nstandardize variables, do regression,\nand form the confidence intervals,\nas if the standardized variables\nwere the original data.\n\nLet us compare the bootstrap confidence\nintervals with the usual OLS confidence\nintervals based on the *t*-statistics.\n\nFor the output of `std_selected_boot()`,\nwe can request the *t*-based confidence\nintervals by setting `type` to\n`\"lm\"` when calling `confint()`, the\nfunction commonly used to form\nconfidence intervals:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# OLS confidence intervals\nround(confint(lm_out_std, type = \"lm\"), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             2.5 % 97.5 %\n(Intercept) -0.094  0.134\ncontrol1    -0.177  0.053\ncontrol2     0.039  0.271\nx            0.277  0.512\nw            0.356  0.584\nx:w          0.083  0.311\n```\n:::\n:::\n\n\nWithout setting `type`, the\nnonparametric bootstrap confidence\nintervals are returned (if available):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap Confidence Intervals\nround(confint(lm_out_std), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             2.5 % 97.5 %\n(Intercept) -0.006  0.052\ncontrol1    -0.153  0.036\ncontrol2     0.045  0.265\nx            0.278  0.503\nw            0.359  0.574\nx:w          0.098  0.289\n```\n:::\n:::\n\n\nAs shown above, the confidence\nintervals of `x`, `w`, and\n`x:w` by the two methods\nare close to each other. However,\nthe bootstrap confidence intervals\ntend to be narrower than the\n*t*-based confidence intervals.\n\nWe can compare the widths of the\nconfidence intervals by examining\ntheir ratios (`ratio` = `CI_Width_t` / `CI_Width_boot`):\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nci_t <- confint(lm_out_std, type = \"lm\")\nci_b <- confint(lm_out_std)\nwidth_t <- ci_t[, 2] - ci_t[, 1]\nwidth_b <- ci_b[, 2] - ci_b[, 1]\nci_compare <- data.frame(CI_Width_t = width_t,\n                         CI_Width_boot = width_b)\nci_compare$ratio <- ci_compare[, 1] / ci_compare[, 2]\nround(ci_compare, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            CI_Width_t CI_Width_boot ratio\n(Intercept)      0.227         0.058 3.918\ncontrol1         0.230         0.189 1.218\ncontrol2         0.232         0.220 1.056\nx                0.235         0.225 1.044\nw                0.229         0.215 1.064\nx:w              0.228         0.191 1.193\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\nThe *t*-based\nconfidence interval of the product\nterm, the standardized moderation\neffect, is nearly 19.3% wider\nthan the bootstrap confidence interval.\n\n# Final Remarks\n\nWhat `std_selected_boot()` does can be\nimplemented in R code by researchers using\nbase R packages only.\nThe function was developed to make\nit easier for researchers to do\nstandardization right in moderated\nregression, without adopting\nthe all-or-none approach.\n\nMore information on `std_selected_boot()`\ncan be found from its\n[help page](https://sfcheung.github.io/stdmod/reference/std_selected.html).\n\n# Other Resources\n\nThere are programs that advocate the\ncorrect way to do standardization\nin moderated regression. For example,\nusers are recommended to standardize\nvariables before using them to do\nmoderated regression. Done this way,\nthey yield correct point estimates\nfor standardized solution\nas `std_selected_boot()` does.\n\nHowever, I am not aware of program that\ndo the bootstrapping correctly when\nforming the bootstrapping confidence\ninterval for the correctly standardized\nsolution. It is because *both* the\nmultiple regression *and* standardization\nneed to be done in *each* bootstrap\nsample. If\nstandardization\nis done only once, before doing\nbootstrapping, even though the point\nestimates are correct, the bootstrap\nconfidence intervals are not.\n\n# Revision History and Issues\n\nThe revision history of this post can\nbe find in the [GitHub history of\nthe source file](https://github.com/blogonresearch/blogonresearch.github.io/commits/main/posts/std_mod/index.qmd).\n\nFor issues on this post, such as corrections\nand mistakes, please [open an issue](https://github.com/blogonresearch/blogonresearch.github.io/issues)\nfor the GitHub repository for this blog.\nThanks.\n\n# References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}